{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9075b4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n",
      "Text detected at: (721, 954) Size: (43, 8)\n",
      "Text detected at: (662, 954) Size: (56, 8)\n",
      "Text detected at: (603, 954) Size: (41, 6)\n",
      "Text detected at: (540, 954) Size: (40, 6)\n",
      "Text detected at: (231, 954) Size: (44, 8)\n",
      "Text detected at: (155, 954) Size: (41, 8)\n",
      "Text detected at: (18, 21) Size: (894, 926)\n",
      "Page 2 converted to C:/Users/Shreshtha/Downloads//page_2.png\n",
      "Text detected at: (22, 196) Size: (187, 15)\n",
      "Text detected at: (21, 138) Size: (188, 15)\n",
      "Text detected at: (23, 126) Size: (34, 9)\n",
      "Page 3 converted to C:/Users/Shreshtha/Downloads//page_3.png\n",
      "Text detected at: (193, 64) Size: (89, 58)\n",
      "Text detected at: (72, 64) Size: (94, 58)\n",
      "Page 4 converted to C:/Users/Shreshtha/Downloads//page_4.png\n",
      "Text detected at: (125, 191) Size: (30, 14)\n",
      "Text detected at: (14, 188) Size: (73, 17)\n",
      "Text detected at: (169, 115) Size: (37, 6)\n",
      "Text detected at: (21, 62) Size: (681, 322)\n",
      "Text detected at: (245, 21) Size: (453, 37)\n",
      "Page 5 converted to C:/Users/Shreshtha/Downloads//page_5.png\n",
      "Text detected at: (264, 318) Size: (22, 10)\n",
      "Text detected at: (228, 318) Size: (22, 10)\n",
      "Text detected at: (264, 262) Size: (22, 9)\n",
      "Text detected at: (46, 168) Size: (252, 167)\n",
      "Text detected at: (46, 50) Size: (248, 124)\n",
      "Page 6 converted to C:/Users/Shreshtha/Downloads//page_6.png\n",
      "Text detected at: (2, 283) Size: (17, 22)\n",
      "Text detected at: (21, 21) Size: (1014, 564)\n",
      "Page 7 converted to C:/Users/Shreshtha/Downloads//page_7.png\n",
      "Text detected at: (108, 133) Size: (125, 19)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image\n",
    "        image = page.get_pixmap()\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d30b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\clahe.cpp:353: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function '`anonymous-namespace'::CLAHE_Impl::apply'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Labelled_MNS_Sample.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 93\u001b[0m convert_pdf_to_images(pdf_path, output_folder)\n",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mconvert_pdf_to_images\u001b[1;34m(pdf_path, output_folder)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m converted to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Enhance image quality\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m enhance_image_quality(image_path)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Perform layout analysis\u001b[39;00m\n\u001b[0;32m     27\u001b[0m analyze_layout(image_path)\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36menhance_image_quality\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Apply contrast adjustment (optional)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m clahe \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcreateCLAHE(clipLimit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, tileGridSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m---> 42\u001b[0m contrast_adjusted_image \u001b[38;5;241m=\u001b[39m clahe\u001b[38;5;241m.\u001b[39mapply(sharpened_image)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Save the enhanced image\u001b[39;00m\n\u001b[0;32m     45\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(image_path, contrast_adjusted_image)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\clahe.cpp:353: error: (-215:Assertion failed) _src.type() == CV_8UC1 || _src.type() == CV_16UC1 in function '`anonymous-namespace'::CLAHE_Impl::apply'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image\n",
    "        image = page.get_pixmap()\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Enhance image quality\n",
    "        enhance_image_quality(image_path)\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def enhance_image_quality(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Apply denoising\n",
    "    denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
    "    \n",
    "    # Apply sharpening\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    sharpened_image = cv2.filter2D(denoised_image, -1, kernel)\n",
    "    \n",
    "    # Apply contrast adjustment (optional)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_adjusted_image = clahe.apply(sharpened_image)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(image_path, contrast_adjusted_image)\n",
    "    \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f4abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n",
      "Text detected at: (24, 966) Size: (34, 7)\n",
      "Text detected at: (721, 954) Size: (44, 8)\n",
      "Text detected at: (662, 954) Size: (56, 8)\n",
      "Text detected at: (603, 954) Size: (41, 7)\n",
      "Text detected at: (540, 954) Size: (40, 6)\n",
      "Text detected at: (434, 954) Size: (41, 6)\n",
      "Text detected at: (344, 954) Size: (44, 7)\n",
      "Text detected at: (231, 954) Size: (44, 8)\n",
      "Text detected at: (155, 954) Size: (41, 8)\n",
      "Text detected at: (18, 21) Size: (894, 929)\n",
      "Page 2 converted to C:/Users/Shreshtha/Downloads//page_2.png\n",
      "Text detected at: (21, 196) Size: (188, 15)\n",
      "Text detected at: (42, 138) Size: (167, 15)\n",
      "Text detected at: (23, 126) Size: (34, 9)\n",
      "Page 3 converted to C:/Users/Shreshtha/Downloads//page_3.png\n",
      "Text detected at: (193, 64) Size: (89, 58)\n",
      "Text detected at: (72, 64) Size: (94, 58)\n",
      "Page 4 converted to C:/Users/Shreshtha/Downloads//page_4.png\n",
      "Text detected at: (14, 188) Size: (73, 17)\n",
      "Text detected at: (168, 115) Size: (38, 6)\n",
      "Text detected at: (21, 60) Size: (681, 324)\n",
      "Text detected at: (245, 21) Size: (232, 37)\n",
      "Page 5 converted to C:/Users/Shreshtha/Downloads//page_5.png\n",
      "Text detected at: (264, 318) Size: (22, 10)\n",
      "Text detected at: (228, 318) Size: (22, 10)\n",
      "Text detected at: (264, 262) Size: (22, 9)\n",
      "Text detected at: (83, 205) Size: (37, 10)\n",
      "Text detected at: (139, 182) Size: (155, 150)\n",
      "Text detected at: (100, 169) Size: (26, 12)\n",
      "Potential table or form detected at: (46, 168) Size: (45, 167)\n",
      "Potential table or form detected at: (285, 99) Size: (9, 75)\n",
      "Text detected at: (46, 50) Size: (248, 123)\n",
      "Page 6 converted to C:/Users/Shreshtha/Downloads//page_6.png\n",
      "Text detected at: (162, 491) Size: (35, 9)\n",
      "Text detected at: (91, 491) Size: (68, 8)\n",
      "Text detected at: (50, 491) Size: (39, 8)\n",
      "Text detected at: (243, 484) Size: (91, 23)\n",
      "Text detected at: (80, 445) Size: (30, 8)\n",
      "Text detected at: (904, 442) Size: (116, 14)\n",
      "Text detected at: (757, 442) Size: (121, 14)\n",
      "Text detected at: (422, 442) Size: (144, 14)\n",
      "Text detected at: (199, 442) Size: (111, 14)\n",
      "Text detected at: (636, 422) Size: (40, 8)\n",
      "Text detected at: (89, 422) Size: (29, 8)\n",
      "Text detected at: (904, 418) Size: (116, 16)\n",
      "Text detected at: (757, 418) Size: (121, 16)\n",
      "Text detected at: (422, 418) Size: (144, 16)\n",
      "Text detected at: (199, 418) Size: (111, 16)\n",
      "Text detected at: (608, 399) Size: (30, 7)\n",
      "Text detected at: (904, 395) Size: (116, 15)\n",
      "Text detected at: (757, 395) Size: (121, 15)\n",
      "Text detected at: (422, 395) Size: (144, 15)\n",
      "Text detected at: (199, 395) Size: (111, 15)\n",
      "Text detected at: (657, 376) Size: (36, 7)\n",
      "Text detected at: (904, 373) Size: (116, 14)\n",
      "Text detected at: (757, 373) Size: (121, 15)\n",
      "Text detected at: (422, 373) Size: (144, 14)\n",
      "Text detected at: (199, 373) Size: (111, 15)\n",
      "Text detected at: (78, 353) Size: (55, 8)\n",
      "Text detected at: (904, 350) Size: (116, 15)\n",
      "Text detected at: (757, 350) Size: (121, 15)\n",
      "Text detected at: (422, 350) Size: (144, 15)\n",
      "Text detected at: (199, 350) Size: (111, 15)\n",
      "Text detected at: (141, 318) Size: (33, 10)\n",
      "Text detected at: (43, 318) Size: (51, 10)\n",
      "Text detected at: (199, 315) Size: (226, 17)\n",
      "Text detected at: (972, 289) Size: (14, 17)\n",
      "Text detected at: (876, 289) Size: (15, 17)\n",
      "Text detected at: (778, 289) Size: (15, 17)\n",
      "Text detected at: (746, 289) Size: (13, 22)\n",
      "Text detected at: (654, 289) Size: (22, 16)\n",
      "Text detected at: (614, 289) Size: (14, 17)\n",
      "Text detected at: (589, 289) Size: (14, 17)\n",
      "Text detected at: (572, 289) Size: (14, 17)\n",
      "Text detected at: (507, 289) Size: (14, 17)\n",
      "Text detected at: (484, 289) Size: (22, 16)\n",
      "Text detected at: (436, 289) Size: (14, 17)\n",
      "Text detected at: (177, 289) Size: (15, 17)\n",
      "Text detected at: (108, 289) Size: (31, 16)\n",
      "Text detected at: (76, 289) Size: (14, 16)\n",
      "Text detected at: (57, 289) Size: (18, 16)\n",
      "Text detected at: (36, 289) Size: (14, 17)\n",
      "Text detected at: (91, 284) Size: (15, 21)\n",
      "Text detected at: (912, 283) Size: (17, 22)\n",
      "Text detected at: (840, 283) Size: (18, 23)\n",
      "Text detected at: (803, 283) Size: (19, 22)\n",
      "Text detected at: (725, 283) Size: (17, 23)\n",
      "Text detected at: (630, 283) Size: (14, 23)\n",
      "Text detected at: (146, 283) Size: (30, 22)\n",
      "Text detected at: (2, 283) Size: (17, 22)\n",
      "Text detected at: (199, 247) Size: (234, 64)\n",
      "Text detected at: (43, 227) Size: (36, 10)\n",
      "Text detected at: (59, 204) Size: (34, 11)\n",
      "Text detected at: (199, 201) Size: (227, 26)\n",
      "Text detected at: (110, 169) Size: (32, 10)\n",
      "Text detected at: (199, 167) Size: (227, 14)\n",
      "Text detected at: (844, 144) Size: (175, 14)\n",
      "Text detected at: (199, 144) Size: (227, 15)\n",
      "Text detected at: (640, 140) Size: (128, 22)\n",
      "Text detected at: (857, 136) Size: (33, 7)\n",
      "Text detected at: (43, 124) Size: (29, 8)\n",
      "Text detected at: (710, 100) Size: (85, 12)\n",
      "Text detected at: (273, 100) Size: (60, 12)\n",
      "Text detected at: (69, 67) Size: (26, 10)\n",
      "Text detected at: (478, 43) Size: (33, 8)\n",
      "Text detected at: (879, 32) Size: (56, 10)\n",
      "Text detected at: (777, 32) Size: (69, 10)\n",
      "Text detected at: (542, 21) Size: (34, 7)\n",
      "Text detected at: (452, 21) Size: (152, 11)\n",
      "Text detected at: (21, 21) Size: (1014, 564)\n",
      "Page 7 converted to C:/Users/Shreshtha/Downloads//page_7.png\n",
      "Text detected at: (108, 133) Size: (125, 19)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image\n",
    "        image = page.get_pixmap()\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Enhance image quality\n",
    "        enhance_image_quality(image_path)\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def enhance_image_quality(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply denoising\n",
    "    denoised_image = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_adjusted_image = clahe.apply(denoised_image)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(image_path, contrast_adjusted_image)\n",
    "    \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2323d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n",
      "Text detected at: (24, 966) Size: (34, 7)\n",
      "Text detected at: (721, 954) Size: (44, 8)\n",
      "Text detected at: (662, 954) Size: (56, 8)\n",
      "Text detected at: (603, 954) Size: (41, 7)\n",
      "Text detected at: (540, 954) Size: (40, 6)\n",
      "Text detected at: (434, 954) Size: (41, 6)\n",
      "Text detected at: (344, 954) Size: (44, 7)\n",
      "Text detected at: (231, 954) Size: (44, 8)\n",
      "Text detected at: (155, 954) Size: (41, 8)\n",
      "Text detected at: (18, 21) Size: (894, 929)\n",
      "Page 2 converted to C:/Users/Shreshtha/Downloads//page_2.png\n",
      "Text detected at: (21, 196) Size: (188, 15)\n",
      "Text detected at: (42, 138) Size: (167, 15)\n",
      "Text detected at: (23, 126) Size: (34, 9)\n",
      "Page 3 converted to C:/Users/Shreshtha/Downloads//page_3.png\n",
      "Text detected at: (193, 64) Size: (89, 58)\n",
      "Text detected at: (72, 64) Size: (94, 58)\n",
      "Page 4 converted to C:/Users/Shreshtha/Downloads//page_4.png\n",
      "Text detected at: (14, 188) Size: (73, 17)\n",
      "Text detected at: (168, 115) Size: (38, 6)\n",
      "Text detected at: (21, 60) Size: (681, 324)\n",
      "Text detected at: (245, 21) Size: (232, 37)\n",
      "Page 5 converted to C:/Users/Shreshtha/Downloads//page_5.png\n",
      "Text detected at: (264, 318) Size: (22, 10)\n",
      "Text detected at: (228, 318) Size: (22, 10)\n",
      "Text detected at: (264, 262) Size: (22, 9)\n",
      "Text detected at: (83, 205) Size: (37, 10)\n",
      "Text detected at: (139, 182) Size: (155, 150)\n",
      "Text detected at: (100, 169) Size: (26, 12)\n",
      "Potential table or form detected at: (46, 168) Size: (45, 167)\n",
      "Potential table or form detected at: (285, 99) Size: (9, 75)\n",
      "Text detected at: (46, 50) Size: (248, 123)\n",
      "Page 6 converted to C:/Users/Shreshtha/Downloads//page_6.png\n",
      "Text detected at: (162, 491) Size: (35, 9)\n",
      "Text detected at: (91, 491) Size: (68, 8)\n",
      "Text detected at: (50, 491) Size: (39, 8)\n",
      "Text detected at: (243, 484) Size: (91, 23)\n",
      "Text detected at: (80, 445) Size: (30, 8)\n",
      "Text detected at: (904, 442) Size: (116, 14)\n",
      "Text detected at: (757, 442) Size: (121, 14)\n",
      "Text detected at: (422, 442) Size: (144, 14)\n",
      "Text detected at: (199, 442) Size: (111, 14)\n",
      "Text detected at: (636, 422) Size: (40, 8)\n",
      "Text detected at: (89, 422) Size: (29, 8)\n",
      "Text detected at: (904, 418) Size: (116, 16)\n",
      "Text detected at: (757, 418) Size: (121, 16)\n",
      "Text detected at: (422, 418) Size: (144, 16)\n",
      "Text detected at: (199, 418) Size: (111, 16)\n",
      "Text detected at: (608, 399) Size: (30, 7)\n",
      "Text detected at: (904, 395) Size: (116, 15)\n",
      "Text detected at: (757, 395) Size: (121, 15)\n",
      "Text detected at: (422, 395) Size: (144, 15)\n",
      "Text detected at: (199, 395) Size: (111, 15)\n",
      "Text detected at: (657, 376) Size: (36, 7)\n",
      "Text detected at: (904, 373) Size: (116, 14)\n",
      "Text detected at: (757, 373) Size: (121, 15)\n",
      "Text detected at: (422, 373) Size: (144, 14)\n",
      "Text detected at: (199, 373) Size: (111, 15)\n",
      "Text detected at: (78, 353) Size: (55, 8)\n",
      "Text detected at: (904, 350) Size: (116, 15)\n",
      "Text detected at: (757, 350) Size: (121, 15)\n",
      "Text detected at: (422, 350) Size: (144, 15)\n",
      "Text detected at: (199, 350) Size: (111, 15)\n",
      "Text detected at: (141, 318) Size: (33, 10)\n",
      "Text detected at: (43, 318) Size: (51, 10)\n",
      "Text detected at: (199, 315) Size: (226, 17)\n",
      "Text detected at: (972, 289) Size: (14, 17)\n",
      "Text detected at: (876, 289) Size: (15, 17)\n",
      "Text detected at: (778, 289) Size: (15, 17)\n",
      "Text detected at: (746, 289) Size: (13, 22)\n",
      "Text detected at: (654, 289) Size: (22, 16)\n",
      "Text detected at: (614, 289) Size: (14, 17)\n",
      "Text detected at: (589, 289) Size: (14, 17)\n",
      "Text detected at: (572, 289) Size: (14, 17)\n",
      "Text detected at: (507, 289) Size: (14, 17)\n",
      "Text detected at: (484, 289) Size: (22, 16)\n",
      "Text detected at: (436, 289) Size: (14, 17)\n",
      "Text detected at: (177, 289) Size: (15, 17)\n",
      "Text detected at: (108, 289) Size: (31, 16)\n",
      "Text detected at: (76, 289) Size: (14, 16)\n",
      "Text detected at: (57, 289) Size: (18, 16)\n",
      "Text detected at: (36, 289) Size: (14, 17)\n",
      "Text detected at: (91, 284) Size: (15, 21)\n",
      "Text detected at: (912, 283) Size: (17, 22)\n",
      "Text detected at: (840, 283) Size: (18, 23)\n",
      "Text detected at: (803, 283) Size: (19, 22)\n",
      "Text detected at: (725, 283) Size: (17, 23)\n",
      "Text detected at: (630, 283) Size: (14, 23)\n",
      "Text detected at: (146, 283) Size: (30, 22)\n",
      "Text detected at: (2, 283) Size: (17, 22)\n",
      "Text detected at: (199, 247) Size: (234, 64)\n",
      "Text detected at: (43, 227) Size: (36, 10)\n",
      "Text detected at: (59, 204) Size: (34, 11)\n",
      "Text detected at: (199, 201) Size: (227, 26)\n",
      "Text detected at: (110, 169) Size: (32, 10)\n",
      "Text detected at: (199, 167) Size: (227, 14)\n",
      "Text detected at: (844, 144) Size: (175, 14)\n",
      "Text detected at: (199, 144) Size: (227, 15)\n",
      "Text detected at: (640, 140) Size: (128, 22)\n",
      "Text detected at: (857, 136) Size: (33, 7)\n",
      "Text detected at: (43, 124) Size: (29, 8)\n",
      "Text detected at: (710, 100) Size: (85, 12)\n",
      "Text detected at: (273, 100) Size: (60, 12)\n",
      "Text detected at: (69, 67) Size: (26, 10)\n",
      "Text detected at: (478, 43) Size: (33, 8)\n",
      "Text detected at: (879, 32) Size: (56, 10)\n",
      "Text detected at: (777, 32) Size: (69, 10)\n",
      "Text detected at: (542, 21) Size: (34, 7)\n",
      "Text detected at: (452, 21) Size: (152, 11)\n",
      "Text detected at: (21, 21) Size: (1014, 564)\n",
      "Page 7 converted to C:/Users/Shreshtha/Downloads//page_7.png\n",
      "Text detected at: (108, 133) Size: (125, 19)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image\n",
    "        image = page.get_pixmap()\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Enhance image quality\n",
    "        enhance_image_quality(image_path)\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def enhance_image_quality(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply denoising\n",
    "    denoised_image = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_adjusted_image = clahe.apply(denoised_image)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(image_path, contrast_adjusted_image)\n",
    "    \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6157b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n",
      "Text detected at: (18, 21) Size: (894, 926)\n",
      "Page 2 converted to C:/Users/Shreshtha/Downloads//page_2.png\n",
      "Text detected at: (22, 196) Size: (187, 15)\n",
      "Text detected at: (22, 138) Size: (187, 15)\n",
      "Page 3 converted to C:/Users/Shreshtha/Downloads//page_3.png\n",
      "Text detected at: (193, 64) Size: (89, 58)\n",
      "Text detected at: (73, 64) Size: (93, 58)\n",
      "Page 4 converted to C:/Users/Shreshtha/Downloads//page_4.png\n",
      "Text detected at: (86, 188) Size: (69, 17)\n",
      "Text detected at: (41, 188) Size: (46, 17)\n",
      "Text detected at: (15, 188) Size: (27, 17)\n",
      "Text detected at: (21, 62) Size: (681, 322)\n",
      "Text detected at: (348, 21) Size: (350, 37)\n",
      "Text detected at: (248, 21) Size: (226, 36)\n",
      "Page 5 converted to C:/Users/Shreshtha/Downloads//page_5.png\n",
      "Text detected at: (46, 50) Size: (252, 285)\n",
      "Page 6 converted to C:/Users/Shreshtha/Downloads//page_6.png\n",
      "Text detected at: (21, 21) Size: (1014, 564)\n",
      "Page 7 converted to C:/Users/Shreshtha/Downloads//page_7.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image\n",
    "        image = page.get_pixmap()\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Enhance image quality\n",
    "        enhance_image_quality(image_path)\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def enhance_image_quality(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Apply bilateral filtering for noise reduction\n",
    "    filtered_image = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "    \n",
    "    # Apply sharpening\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    sharpened_image = cv2.filter2D(filtered_image, -1, kernel)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(image_path, sharpened_image)\n",
    "    \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    return blurred\n",
    "\n",
    "# Function to detect text regions\n",
    "def detect_text_regions(image):\n",
    "    # Use text detection algorithms to identify text regions\n",
    "    # For simplicity, let's assume text regions are already known\n",
    "    \n",
    "    # Example: Assuming text regions are provided as a list of bounding boxes\n",
    "    text_regions = [(100, 100, 200, 50), (150, 200, 180, 40)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return text_regions\n",
    "\n",
    "# Function to detect lines and borders\n",
    "def detect_lines_and_borders(image):\n",
    "    # Use line and border detection algorithms\n",
    "    # For simplicity, let's assume lines and borders are already known\n",
    "    \n",
    "    # Example: Assuming lines and borders are provided as a list of bounding boxes\n",
    "    lines_and_borders = [(50, 50, 5, 200), (100, 100, 200, 5)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return lines_and_borders\n",
    "\n",
    "# Function to segment tables\n",
    "def segment_tables(image, lines_and_borders):\n",
    "    # Segment the image into individual tables\n",
    "    # For simplicity, let's assume tables are already known\n",
    "    \n",
    "    # Example: Assuming tables are provided as a list of bounding boxes\n",
    "    tables = [(100, 100, 300, 200)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# Function to identify columns and rows\n",
    "def identify_columns_and_rows(table_image, lines_and_borders):\n",
    "    # Identify columns and rows within a table\n",
    "    # For simplicity, let's assume columns and rows are already known\n",
    "    \n",
    "    # Example: Assuming columns are provided as a list of bounding boxes\n",
    "    columns = [(100, 100, 50, 200)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "# Function to draw boxes for columns\n",
    "def draw_boxes_for_columns(image, columns):\n",
    "    # Draw bounding boxes around identified columns\n",
    "    for column in columns:\n",
    "        x, y, w, h = column\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Function to identify form fields\n",
    "def identify_form_fields(image, text_regions):\n",
    "    # Use contour detection and template matching to identify form fields\n",
    "    # For simplicity, let's assume form fields are already known\n",
    "    \n",
    "    # Example: Assuming form fields are provided as a list of bounding boxes\n",
    "    form_fields = [(150, 300, 100, 30)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return form_fields\n",
    "\n",
    "# Function to draw boxes for key-value pairs\n",
    "def draw_boxes_for_key_value_pairs(image, key_value_pairs):\n",
    "    # Draw bounding boxes around identified form fields\n",
    "    for key_value_pair in key_value_pairs:\n",
    "        x, y, w, h = key_value_pair\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"your_image.jpg\")\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image)\n",
    "\n",
    "# Detect text regions\n",
    "text_regions = detect_text_regions(processed_image)\n",
    "\n",
    "# Detect lines and borders\n",
    "lines_and_borders = detect_lines_and_borders(processed_image)\n",
    "\n",
    "# Segment tables\n",
    "tables = segment_tables(processed_image, lines_and_borders)\n",
    "\n",
    "# Analyze tables\n",
    "for table in tables:\n",
    "    # Identify columns and rows\n",
    "    columns = identify_columns_and_rows(processed_image, lines_and_borders)\n",
    "    \n",
    "    # Draw bounding boxes for columns\n",
    "    draw_boxes_for_columns(processed_image, columns)\n",
    "\n",
    "# Identify form fields\n",
    "form_fields = identify_form_fields(processed_image, text_regions)\n",
    "\n",
    "# Draw bounding boxes for key-value pairs\n",
    "draw_boxes_for_key_value_pairs(processed_image, form_fields)\n",
    "\n",
    "# Display the image with drawn boxes\n",
    "cv2.imshow(\"Image with Boxes\", processed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    return blurred\n",
    "\n",
    "# Function to detect text regions\n",
    "def detect_text_regions(image):\n",
    "    # Use text detection algorithms to identify text regions\n",
    "    # For simplicity, let's assume text regions are already known\n",
    "    \n",
    "    # Example: Assuming text regions are provided as a list of bounding boxes\n",
    "    text_regions = [(100, 100, 200, 50), (150, 200, 180, 40)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return text_regions\n",
    "\n",
    "# Function to detect lines and borders\n",
    "def detect_lines_and_borders(image):\n",
    "    # Use line and border detection algorithms\n",
    "    # For simplicity, let's assume lines and borders are already known\n",
    "    \n",
    "    # Example: Assuming lines and borders are provided as a list of bounding boxes\n",
    "    lines_and_borders = [(50, 50, 5, 200), (100, 100, 200, 5)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return lines_and_borders\n",
    "\n",
    "# Function to segment tables\n",
    "def segment_tables(image, lines_and_borders):\n",
    "    # Segment the image into individual tables\n",
    "    # For simplicity, let's assume tables are already known\n",
    "    \n",
    "    # Example: Assuming tables are provided as a list of bounding boxes\n",
    "    tables = [(100, 100, 300, 200)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# Function to identify columns and rows\n",
    "def identify_columns_and_rows(table_image, lines_and_borders):\n",
    "    # Identify columns and rows within a table\n",
    "    # For simplicity, let's assume columns and rows are already known\n",
    "    \n",
    "    # Example: Assuming columns are provided as a list of bounding boxes\n",
    "    columns = [(100, 100, 50, 200)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "# Function to draw boxes for columns\n",
    "def draw_boxes_for_columns(image, columns):\n",
    "    # Draw bounding boxes around identified columns\n",
    "    for column in columns:\n",
    "        x, y, w, h = column\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Function to identify form fields\n",
    "def identify_form_fields(image, text_regions):\n",
    "    # Use contour detection and template matching to identify form fields\n",
    "    # For simplicity, let's assume form fields are already known\n",
    "    \n",
    "    # Example: Assuming form fields are provided as a list of bounding boxes\n",
    "    form_fields = [(150, 300, 100, 30)]  # Format: (x, y, width, height)\n",
    "    \n",
    "    return form_fields\n",
    "\n",
    "# Function to draw boxes for key-value pairs\n",
    "def draw_boxes_for_key_value_pairs(image, key_value_pairs):\n",
    "    # Draw bounding boxes around identified form fields\n",
    "    for key_value_pair in key_value_pairs:\n",
    "        x, y, w, h = key_value_pair\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# Directory containing input images\n",
    "input_dir = \"input_images\"\n",
    "\n",
    "# Directory to save output images\n",
    "output_dir = \"output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(input_dir, filename))\n",
    "    \n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_image(image)\n",
    "\n",
    "    # Detect text regions\n",
    "    text_regions = detect_text_regions(processed_image)\n",
    "\n",
    "    # Detect lines and borders\n",
    "    lines_and_borders = detect_lines_and_borders(processed_image)\n",
    "\n",
    "    # Segment tables\n",
    "    tables = segment_tables(processed_image, lines_and_borders)\n",
    "\n",
    "    # Analyze tables\n",
    "    for table in tables:\n",
    "        # Identify columns and rows\n",
    "        columns = identify_columns_and_rows(processed_image, lines_and_borders)\n",
    "        \n",
    "        # Draw bounding boxes for columns\n",
    "        draw_boxes_for_columns(processed_image, columns)\n",
    "\n",
    "    # Identify form fields\n",
    "    form_fields = identify_form_fields(processed_image, text_regions)\n",
    "\n",
    "    # Draw bounding boxes for key-value pairs\n",
    "    draw_boxes_for_key_value_pairs(processed_image, form_fields)\n",
    "\n",
    "    # Save the processed image\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, processed_image)\n",
    "\n",
    "    print(f\"Processed image saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1128ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pixmap' object has no attribute 'writePNG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     pdf_document\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m pdf_to_images(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Labelled_MNS_Sample.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mpdf_to_images\u001b[1;34m(pdf_path, output_folder)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Save the Pixmap as an image\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/page_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m     pixmap\u001b[38;5;241m.\u001b[39mwritePNG(image_path)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Close the PDF\u001b[39;00m\n\u001b[0;32m     20\u001b[0m pdf_document\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pixmap' object has no attribute 'writePNG'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "\n",
    "        # Convert the page to a Pixmap with higher DPI\n",
    "        pixmap = page.get_pixmap(matrix=fitz.Matrix(600/72, 600/72))\n",
    "\n",
    "        # Save the Pixmap as an image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        pixmap.writePNG(image_path)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Example usage\n",
    "pdf_to_images(\"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\", \"C:/Users/Shreshtha/Downloads/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382aace1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pixmap' object has no attribute 'write_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     pdf_document\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m pdf_to_images(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Labelled_MNS_Sample.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mpdf_to_images\u001b[1;34m(pdf_path, output_folder)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Save the Pixmap as an image\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     pixmap\u001b[38;5;241m.\u001b[39mwrite_image(image_path)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Close the PDF\u001b[39;00m\n\u001b[0;32m     24\u001b[0m pdf_document\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pixmap' object has no attribute 'write_image'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "\n",
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "\n",
    "        # Convert the page to a Pixmap with higher DPI\n",
    "        pixmap = page.get_pixmap(matrix=fitz.Matrix(600/72, 600/72))\n",
    "\n",
    "        # Save the Pixmap as an image\n",
    "        image_path = os.path.join(output_folder, f\"page_{page_number + 1}.png\")\n",
    "        pixmap.write_image(image_path)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "# Example usage\n",
    "pdf_to_images(\"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\", \"C:/Users/Shreshtha/Downloads/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daafcba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 converted to C:/Users/Shreshtha/Downloads//page_1.png\n",
      "Text detected at: (18, 21) Size: (894, 926)\n",
      "Page 2 converted to C:/Users/Shreshtha/Downloads//page_2.png\n",
      "Text detected at: (21, 196) Size: (188, 15)\n",
      "Text detected at: (21, 138) Size: (188, 15)\n",
      "Page 3 converted to C:/Users/Shreshtha/Downloads//page_3.png\n",
      "Text detected at: (193, 64) Size: (89, 58)\n",
      "Text detected at: (72, 64) Size: (94, 58)\n",
      "Page 4 converted to C:/Users/Shreshtha/Downloads//page_4.png\n",
      "Text detected at: (14, 188) Size: (73, 17)\n",
      "Text detected at: (169, 115) Size: (37, 6)\n",
      "Text detected at: (21, 62) Size: (681, 322)\n",
      "Text detected at: (245, 21) Size: (453, 37)\n",
      "Page 5 converted to C:/Users/Shreshtha/Downloads//page_5.png\n",
      "Text detected at: (46, 168) Size: (248, 167)\n",
      "Text detected at: (46, 50) Size: (248, 124)\n",
      "Page 6 converted to C:/Users/Shreshtha/Downloads//page_6.png\n",
      "Text detected at: (162, 491) Size: (32, 8)\n",
      "Text detected at: (244, 484) Size: (89, 22)\n",
      "Text detected at: (904, 442) Size: (115, 14)\n",
      "Text detected at: (758, 442) Size: (120, 14)\n",
      "Text detected at: (423, 442) Size: (143, 14)\n",
      "Text detected at: (199, 442) Size: (111, 14)\n",
      "Text detected at: (636, 422) Size: (32, 8)\n",
      "Text detected at: (904, 419) Size: (115, 15)\n",
      "Text detected at: (758, 419) Size: (120, 15)\n",
      "Text detected at: (423, 419) Size: (143, 15)\n",
      "Text detected at: (199, 419) Size: (111, 15)\n",
      "Text detected at: (904, 396) Size: (115, 14)\n",
      "Text detected at: (758, 396) Size: (120, 14)\n",
      "Text detected at: (423, 396) Size: (143, 14)\n",
      "Text detected at: (199, 396) Size: (111, 14)\n",
      "Text detected at: (904, 373) Size: (115, 14)\n",
      "Text detected at: (758, 373) Size: (120, 14)\n",
      "Text detected at: (423, 373) Size: (143, 14)\n",
      "Text detected at: (199, 373) Size: (111, 14)\n",
      "Text detected at: (904, 350) Size: (115, 14)\n",
      "Text detected at: (758, 350) Size: (120, 14)\n",
      "Text detected at: (423, 350) Size: (143, 14)\n",
      "Text detected at: (199, 350) Size: (111, 14)\n",
      "Text detected at: (43, 318) Size: (29, 10)\n",
      "Text detected at: (199, 316) Size: (226, 16)\n",
      "Text detected at: (972, 289) Size: (14, 17)\n",
      "Text detected at: (876, 289) Size: (15, 17)\n",
      "Text detected at: (778, 289) Size: (15, 16)\n",
      "Text detected at: (746, 289) Size: (13, 22)\n",
      "Text detected at: (614, 289) Size: (14, 16)\n",
      "Text detected at: (589, 289) Size: (14, 17)\n",
      "Text detected at: (572, 289) Size: (14, 17)\n",
      "Text detected at: (507, 289) Size: (14, 17)\n",
      "Text detected at: (436, 289) Size: (14, 16)\n",
      "Text detected at: (177, 289) Size: (15, 17)\n",
      "Text detected at: (124, 289) Size: (15, 16)\n",
      "Text detected at: (108, 289) Size: (16, 16)\n",
      "Text detected at: (76, 289) Size: (14, 16)\n",
      "Text detected at: (57, 289) Size: (18, 16)\n",
      "Text detected at: (36, 289) Size: (14, 16)\n",
      "Text detected at: (91, 284) Size: (16, 21)\n",
      "Text detected at: (913, 283) Size: (16, 22)\n",
      "Text detected at: (840, 283) Size: (18, 23)\n",
      "Text detected at: (803, 283) Size: (19, 22)\n",
      "Text detected at: (725, 283) Size: (17, 23)\n",
      "Text detected at: (630, 283) Size: (14, 22)\n",
      "Text detected at: (148, 283) Size: (28, 22)\n",
      "Text detected at: (2, 283) Size: (17, 22)\n",
      "Text detected at: (199, 247) Size: (234, 64)\n",
      "Text detected at: (43, 228) Size: (36, 9)\n",
      "Text detected at: (199, 201) Size: (226, 26)\n",
      "Text detected at: (199, 167) Size: (226, 14)\n",
      "Text detected at: (844, 144) Size: (175, 14)\n",
      "Text detected at: (199, 144) Size: (226, 14)\n",
      "Text detected at: (640, 140) Size: (127, 22)\n",
      "Text detected at: (43, 125) Size: (29, 7)\n",
      "Text detected at: (879, 33) Size: (56, 9)\n",
      "Text detected at: (778, 32) Size: (68, 10)\n",
      "Text detected at: (452, 21) Size: (152, 10)\n",
      "Text detected at: (21, 21) Size: (1014, 564)\n",
      "Page 7 converted to C:/Users/Shreshtha/Downloads//page_7.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Convert the page to a PIL image with higher DPI\n",
    "        zoom = 2.0  # Adjust the zoom factor as needed for higher resolution\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Save the image\n",
    "        image_path = f\"{output_folder}/page_{page_number + 1}.png\"\n",
    "        pix.save(image_path)\n",
    "        \n",
    "        print(f\"Page {page_number + 1} converted to {image_path}\")\n",
    "        \n",
    "        # Enhance image quality\n",
    "        enhance_image_quality(image_path)\n",
    "        \n",
    "        # Perform layout analysis\n",
    "        analyze_layout(image_path)\n",
    "        \n",
    "def enhance_image_quality(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Apply Gaussian blur for noise reduction and smoother edges\n",
    "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    \n",
    "    # Apply unsharp masking for enhancing details\n",
    "    unsharp_image = cv2.addWeighted(image, 1.5, blurred_image, -0.5, 0)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(image_path, unsharp_image)\n",
    "    \n",
    "def analyze_layout(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 4)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Iterate through contours\n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # If contour area is small, skip\n",
    "        if area < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding rectangle (for visualization)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Classify contour based on aspect ratio\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if 0.5 < aspect_ratio < 20:\n",
    "            print(\"Text detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the text region here\n",
    "        elif 0.1 < aspect_ratio < 5:\n",
    "            print(\"Potential table or form detected at:\", (x, y), \"Size:\", (w, h))\n",
    "            # You can further process the table or form region here\n",
    "    \n",
    "    # Display the image with contours\n",
    "    cv2.imshow(\"Layout Analysis\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"C:/Users/Shreshtha/Labelled_MNS_Sample.pdf\"\n",
    "output_folder = \"C:/Users/Shreshtha/Downloads/\"\n",
    "convert_pdf_to_images(pdf_path, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
