{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75cdcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\shreshtha\\anaconda3\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (10.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (2.1.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\shreshtha\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Collecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.2 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.2 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.2 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/61.2 kB 119.8 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/61.2 kB 187.9 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/61.2 kB 187.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.2/61.2 kB 181.5 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\shreshtha\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Collecting pymsgbox (from pyautogui)\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Downloading pytweening-1.0.7.tar.gz (168 kB)\n",
      "     ---------------------------------------- 0.0/168.2 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/168.2 kB ? eta -:--:--\n",
      "     --------- --------------------------- 41.0/168.2 kB 495.5 kB/s eta 0:00:01\n",
      "     ------------- ----------------------- 61.4/168.2 kB 409.6 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 92.2/168.2 kB 525.1 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 143.4/168.2 kB 568.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 168.2/168.2 kB 631.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Downloading PyScreeze-0.1.30.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mouseinfo (from pyautogui)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml): started\n",
      "  Building wheel for pyautogui (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37597 sha256=0c174a60ffd058d7a6941ab81ffa17d2bcc7a73bc53eabf91bbfe7ae14a776f1\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\95\\dc\\b1\\fe122b791e0db8bf439a0e6e1d2628e48f10bf430cae13521b\n",
      "  Building wheel for pygetwindow (setup.py): started\n",
      "  Building wheel for pygetwindow (setup.py): finished with status 'done'\n",
      "  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11079 sha256=b4a13cb5a2532b5ed225834eabc492b63f419ad7cbaae341fc53881b75b50e30\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\07\\75\\0b\\7ca0b598eb4c21d43ba4bcc78a0538dfcf803a5997da33bc19\n",
      "  Building wheel for pyscreeze (pyproject.toml): started\n",
      "  Building wheel for pyscreeze (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyscreeze: filename=PyScreeze-0.1.30-py3-none-any.whl size=14400 sha256=a99e2453a06a9101fc66c5db0b9bb87f2128c20fe79834f17103fca3572c672a\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\df\\bc\\15\\d685ca085ca4b11e46e54cc3da4e501a98856c7fea8f604500\n",
      "  Building wheel for pytweening (setup.py): started\n",
      "  Building wheel for pytweening (setup.py): finished with status 'done'\n",
      "  Created wheel for pytweening: filename=pytweening-1.0.7-py3-none-any.whl size=6214 sha256=a181ca1419711c586ae57818dd651708d939159c7a7f5376c1f222d647c9d63d\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\b2\\9b\\02\\059beba389e7e31a635bd9e8d9b7299f4ec11caca1f237f56d\n",
      "  Building wheel for mouseinfo (setup.py): started\n",
      "  Building wheel for mouseinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10906 sha256=a16903e1c6ce86c25d3c6d80b3abdf990baae2372c56f6c2ce92a7a98333a372\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\20\\0b\\7f\\939ac9ff785b09951c706150537572c00123412f260a6024f3\n",
      "  Building wheel for pymsgbox (pyproject.toml): started\n",
      "  Building wheel for pymsgbox (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7416 sha256=2a71a4e77d0bc63c2a38e34c45eab126e0c9158267d0701400c2b05fd7051b83\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\85\\92\\63\\e126ee5f33d8f2ed04f96e43ef5df7270a2f331848752e8662\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=b2f678af93b1ee04a83ff28123cec82263ba91dab1c0448e56cee3bbc0a992d8\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\70\\bd\\ba\\8ae5c080c895c9360fe6e153acda2dee82527374467eae061b\n",
      "  Building wheel for pyrect (setup.py): started\n",
      "  Building wheel for pyrect (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11205 sha256=d093a35b62400179916c59ca083dcb69f148aff8910b99fc2688908075fceec0\n",
      "  Stored in directory: c:\\users\\shreshtha\\appdata\\local\\pip\\cache\\wheels\\c4\\e9\\fc\\b7a666dd4f9a3168fb44d643079b41d36ddab52f470707e820\n",
      "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n",
      "Installing collected packages: pytweening, pyrect, pyperclip, pymsgbox, pyscreeze, pygetwindow, mouseinfo, pyautogui\n",
      "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.8.2 pyrect-0.2.0 pyscreeze-0.1.30 pytweening-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract Pillow pandas openpyxl pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336a6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76fc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9715bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    # Create a new Excel workbook\n",
    "    new_workbook = pd.ExcelWriter(output_path, engine='openpyxl')\n",
    "    new_workbook.book.save(output_path)\n",
    "    writer = pd.ExcelWriter(output_path, engine='openpyxl', mode='a')\n",
    "\n",
    "    # Load the existing workbook\n",
    "    existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "    # Iterate through each sheet and copy data to the new workbook\n",
    "    for sheet_name in existing_workbook.sheetnames:\n",
    "        df = pd.read_excel(existing_workbook, sheet_name)\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "        # Extract text from images in the current sheet\n",
    "        sheet = existing_workbook[sheet_name]\n",
    "        for image in sheet._images:\n",
    "            img = Image.open(image.image)\n",
    "            text_data = extract_text_from_image(img)\n",
    "            if text_data:\n",
    "                # Split text data into lines\n",
    "                lines = text_data.split('\\n')\n",
    "\n",
    "                # Dynamically create DataFrame based on the number of columns\n",
    "                max_cols = max(len(line.split('\\t')) for line in lines)\n",
    "                columns = [f\"Column{i}\" for i in range(1, max_cols + 1)]\n",
    "                df_img = pd.DataFrame([line.split('\\t') for line in lines], columns=columns)\n",
    "\n",
    "                # Append the text data to the new workbook\n",
    "                df_img.to_excel(writer, sheet_name='CombinedData', index=False, header=False)\n",
    "\n",
    "    # Save the new workbook\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e192132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users//Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c28733",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "At least one sheet must be visible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine structured data from Excel sheets and extract text from images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcombine_excel_sheets\u001b[1;34m(workbook_path, output_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_excel_sheets\u001b[39m(workbook_path, output_path):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Create a new Excel workbook\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     new_workbook \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mExcelWriter(output_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     new_workbook\u001b[38;5;241m.\u001b[39mbook\u001b[38;5;241m.\u001b[39msave(output_path)\n\u001b[0;32m      6\u001b[0m     writer \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mExcelWriter(output_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Load the existing workbook\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:407\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[1;32m--> 407\u001b[0m save_workbook(\u001b[38;5;28mself\u001b[39m, filename)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:293\u001b[0m, in \u001b[0;36msave_workbook\u001b[1;34m(workbook, filename)\u001b[0m\n\u001b[0;32m    291\u001b[0m archive \u001b[38;5;241m=\u001b[39m ZipFile(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, ZIP_DEFLATED, allowZip64\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    292\u001b[0m writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n\u001b[1;32m--> 293\u001b[0m writer\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:275\u001b[0m, in \u001b[0;36mExcelWriter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_data()\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:89\u001b[0m, in \u001b[0;36mExcelWriter.write_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m writer \u001b[38;5;241m=\u001b[39m WorkbookWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkbook)\n\u001b[0;32m     88\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_ROOT_RELS, writer\u001b[38;5;241m.\u001b[39mwrite_root_rels())\n\u001b[1;32m---> 89\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_WORKBOOK, writer\u001b[38;5;241m.\u001b[39mwrite())\n\u001b[0;32m     90\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_WORKBOOK_RELS, writer\u001b[38;5;241m.\u001b[39mwrite_rels())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_vba()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:148\u001b[0m, in \u001b[0;36mWorkbookWriter.write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_names()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_pivots()\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_views()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_refs()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tostring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpackage\u001b[38;5;241m.\u001b[39mto_tree())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:135\u001b[0m, in \u001b[0;36mWorkbookWriter.write_views\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_views\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 135\u001b[0m     active \u001b[38;5;241m=\u001b[39m get_active_sheet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb\u001b[38;5;241m.\u001b[39mviews:\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb\u001b[38;5;241m.\u001b[39mviews[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactiveTab \u001b[38;5;241m=\u001b[39m active\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:33\u001b[0m, in \u001b[0;36mget_active_sheet\u001b[1;34m(wb)\u001b[0m\n\u001b[0;32m     31\u001b[0m visible_sheets \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, sheet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wb\u001b[38;5;241m.\u001b[39m_sheets) \u001b[38;5;28;01mif\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39msheet_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisible\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m visible_sheets:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one sheet must be visible\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m idx \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39m_active_sheet_index\n\u001b[0;32m     36\u001b[0m sheet \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39mactive\n",
      "\u001b[1;31mIndexError\u001b[0m: At least one sheet must be visible"
     ]
    }
   ],
   "source": [
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2d4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error combining sheets: At least one sheet must be visible\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook\n",
    "        new_workbook = pd.ExcelWriter(output_path, engine='openpyxl')\n",
    "        new_workbook.book.save(output_path)\n",
    "        writer = pd.ExcelWriter(output_path, engine='openpyxl', mode='a')\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new workbook\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            df = pd.read_excel(existing_workbook, sheet_name)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            sheet = existing_workbook[sheet_name]\n",
    "            print(f\"Number of images in sheet: {len(sheet.drawing_images)}\")\n",
    "            \n",
    "            for image in sheet.drawing_images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Dynamically create DataFrame based on the number of columns\n",
    "                        max_cols = max(len(line.split('\\t')) for line in lines)\n",
    "                        columns = [f\"Column{i}\" for i in range(1, max_cols + 1)]\n",
    "                        \n",
    "                        df_img = pd.DataFrame([line.split('\\t') for line in lines], columns=columns)\n",
    "                        df_img.to_excel(writer, sheet_name='CombinedData', index=False, header=False)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "    \n",
    "        # Save the new workbook\n",
    "        writer.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa0a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error combining sheets: \"There is no item named '[Content_Types].xml' in the archive\"\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook\n",
    "        new_workbook = Workbook()\n",
    "        new_writer = pd.ExcelWriter(output_path, engine='openpyxl', mode='a')\n",
    "        new_workbook.save(output_path)\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new workbook\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            df = pd.read_excel(existing_workbook, sheet_name)\n",
    "            df.to_excel(new_writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            sheet = existing_workbook[sheet_name]\n",
    "            print(f\"Number of images in sheet: {len(sheet.drawing_images)}\")\n",
    "            \n",
    "            for image in sheet.drawing_images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Dynamically create DataFrame based on the number of columns\n",
    "                        max_cols = max(len(line.split('\\t')) for line in lines)\n",
    "                        columns = [f\"Column{i}\" for i in range(1, max_cols + 1)]\n",
    "                        \n",
    "                        df_img = pd.DataFrame([line.split('\\t') for line in lines], columns=columns)\n",
    "                        df_img.to_excel(new_writer, sheet_name='CombinedData', index=False, header=False)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "\n",
    "        # Save the new workbook\n",
    "        new_writer.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd377c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Obtaining dependency information for xlsxwriter from https://files.pythonhosted.org/packages/f7/3e/05ba2194cd5073602422859c949a4f21310a3c49bf8dccde9e03d4522b11/XlsxWriter-3.1.9-py3-none-any.whl.metadata\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/154.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/154.8 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 61.4/154.8 kB 465.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 122.9/154.8 kB 798.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 154.8/154.8 kB 768.7 kB/s eta 0:00:00\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded37a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: Sheet1\n",
      "Error combining sheets: Invalid file path or buffer object type: <class 'openpyxl.workbook.workbook.Workbook'>\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import xlsxwriter  # Import the xlsxwriter engine\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook using pandas and xlsxwriter engine\n",
    "        new_workbook = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
    "        new_workbook.book.close()\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new workbook\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            df = pd.read_excel(existing_workbook, sheet_name)\n",
    "            df.to_excel(new_workbook, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            sheet = existing_workbook[sheet_name]\n",
    "            print(f\"Number of images in sheet: {len(sheet.drawing_images)}\")\n",
    "            \n",
    "            for image in sheet.drawing_images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Dynamically create DataFrame based on the number of columns\n",
    "                        max_cols = max(len(line.split('\\t')) for line in lines)\n",
    "                        columns = [f\"Column{i}\" for i in range(1, max_cols + 1)]\n",
    "                        \n",
    "                        df_img = pd.DataFrame([line.split('\\t') for line in lines], columns=columns)\n",
    "                        df_img.to_excel(new_workbook, sheet_name='CombinedData', index=False, header=False)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "\n",
    "        # Save the new workbook\n",
    "        new_workbook.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a2290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: Sheet1\n",
      "Error combining sheets: Invalid file path or buffer object type: <class 'openpyxl.workbook.workbook.Workbook'>\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook using openpyxl\n",
    "        new_workbook = Workbook()\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new workbook\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            df = pd.read_excel(existing_workbook, sheet_name)\n",
    "            df.to_excel(new_workbook, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            sheet = existing_workbook[sheet_name]\n",
    "            print(f\"Number of images in sheet: {len(sheet.drawing_images)}\")\n",
    "            \n",
    "            for image in sheet.drawing_images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Dynamically create DataFrame based on the number of columns\n",
    "                        max_cols = max(len(line.split('\\t')) for line in lines)\n",
    "                        columns = [f\"Column{i}\" for i in range(1, max_cols + 1)]\n",
    "                        \n",
    "                        df_img = pd.DataFrame([line.split('\\t') for line in lines], columns=columns)\n",
    "                        df_img.to_excel(new_workbook, sheet_name='CombinedData', index=False, header=False)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "\n",
    "        # Save the new workbook\n",
    "        new_workbook.save(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c14267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: Sheet1\n",
      "Error combining sheets: 'Worksheet' object has no attribute 'drawing_images'\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook\n",
    "        new_workbook = Workbook()\n",
    "\n",
    "        # Create a new sheet in the new workbook\n",
    "        new_sheet = new_workbook.active\n",
    "        new_sheet.title = 'CombinedData'\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new sheet\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            existing_sheet = existing_workbook[sheet_name]\n",
    "\n",
    "            # Copy data from existing sheet to new sheet\n",
    "            for row in existing_sheet.iter_rows(values_only=True):\n",
    "                new_sheet.append(row)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            print(f\"Number of images in sheet: {len(existing_sheet.drawing_images)}\")\n",
    "            for image in existing_sheet.drawing_images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Append text data to the new sheet\n",
    "                        new_sheet.append(lines)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "\n",
    "        # Save the new workbook\n",
    "        new_workbook.save(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e6323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: Sheet1\n",
      "Error combining sheets: 'Worksheet' object has no attribute 'drawing'\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = image.convert('L')\n",
    "        \n",
    "        # Use Tesseract OCR to extract text from the image\n",
    "        text = pytesseract.image_to_string(gray_image)\n",
    "        \n",
    "        # Strip any leading or trailing whitespace from the extracted text\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine structured data from Excel sheets\n",
    "def combine_excel_sheets(workbook_path, output_path):\n",
    "    try:\n",
    "        # Create a new Excel workbook\n",
    "        new_workbook = Workbook()\n",
    "\n",
    "        # Create a new sheet in the new workbook\n",
    "        new_sheet = new_workbook.active\n",
    "        new_sheet.title = 'CombinedData'\n",
    "\n",
    "        # Load the existing workbook\n",
    "        existing_workbook = load_workbook(workbook_path, read_only=False)\n",
    "\n",
    "        # Iterate through each sheet and copy data to the new sheet\n",
    "        for sheet_name in existing_workbook.sheetnames:\n",
    "            print(f\"Sheet name: {sheet_name}\")\n",
    "            existing_sheet = existing_workbook[sheet_name]\n",
    "\n",
    "            # Copy data from existing sheet to new sheet\n",
    "            for row in existing_sheet.iter_rows(values_only=True):\n",
    "                new_sheet.append(row)\n",
    "\n",
    "            # Extract text from images in the current sheet\n",
    "            drawing = existing_sheet.drawing\n",
    "            print(f\"Number of images in sheet: {len(drawing.images)}\")\n",
    "            for image in drawing.images:\n",
    "                try:\n",
    "                    img = Image.open(image.image)\n",
    "                    text_data = extract_text_from_image(img)\n",
    "                    \n",
    "                    if text_data:\n",
    "                        lines = text_data.split('\\n')\n",
    "                        \n",
    "                        # Append text data to the new sheet\n",
    "                        new_sheet.append(lines)\n",
    "                        \n",
    "                except Exception as img_error:\n",
    "                    print(f\"Error processing image: {img_error}\")\n",
    "\n",
    "        # Save the new workbook\n",
    "        new_workbook.save(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining sheets: {e}\")\n",
    "\n",
    "# Paths to your files\n",
    "structured_data_workbook_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "combined_output_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/CombinedDataWorkbook.xlsx\"\n",
    "\n",
    "# Combine structured data from Excel sheets and extract text from images\n",
    "combine_excel_sheets(structured_data_workbook_path, combined_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9d17cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Heme: Aurora Products\n",
      "Group Number (or TBD): 1264561\n",
      "Request Date: 7214/2020\n",
      "Effective Date of NSB: 31/2020\n",
      "\n",
      "Please Note: All tems isied on this form are subject to review, Please highlight the NSB's in yellow,\n",
      "\n",
      "Benefits Currently in Place Requested Benefits\n",
      "\n",
      "Current Plan 4: Current Plan 2:\n",
      "0100001 BUY UP 0100002 CORE\n",
      "\n",
      "Base (Similar standard Plan) Tracking ID # (REQUIRED)\n",
      "+ Reach out to your Underwriter\n",
      "\n",
      "+ 4 14\n",
      "# of Employees Enrolled: 39 2 25 % 26 15\n",
      "Group State (NY, NJ, CT) cr cr cr. cr cr cr\n",
      "Market (Large or Small Group) Large Large Large Large Large Large\n",
      "Product (e.9: HHO, POS, Classic, Access, Direct, OxUSA, Value Option\n",
      "or EPO) HMO Huo HSA Huo HMO HSA\n",
      "Access (Gated or Non-Gated) Non-Gated Gated Non-Gated Non-Gated Gates Non-Gated\n",
      "Network (Freedom, Liberty, or Choice Pius) Freedom Freedom Freedom Freedom Freedom Freedom\n",
      "Current Carrier (Oxford or Competitor) Polaris Polaris Polaris Polaris\n",
      "\n",
      "Nore INE\n",
      "PCPISpecialist OV Copay 30/50 30/50 D8 100%\n",
      "ER Cost Share 450 150 pac\n",
      "IP: $260/Daq-$1500 Max (P: $250/0ay-$1500 Max\n",
      "after deductible ‘after deductible,\n",
      "OP:FS-Ded.then OP: FS-Ded,then\n",
      "\n",
      "$260Hisit  Hosp-Ded, then] $260Hvisit (Hosp Ded, then|\n",
      "Hospital Cost Share (Inpstient/Outpatient ‘$5000 $00 isi\n",
      "\n",
      "For Direct, EPO or Valve Option Products:\n",
      "IN Deductible - Individual & Family 2500/5000 2500/5000 5000/1000 2500/5000 2500/5000 5000/1000\n",
      "IN Coinsurance % (100, 90/10, 80/20 1000 4100/0 soo 4100/0 10070 90/10\n",
      "\n",
      "IN Coinsurance Limit ($Sk, $10k..) Indiv. & Fam.\n",
      "\n",
      "IN Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. 3000/6000 5000/1000 6250/12500 3000/6000 5000/1000 6250/12500\n",
      "Qut of Network (OON):\n",
      "\n",
      "OON Deductible - individual & Family NA NA. 5000/1000 NA. NA '5000/10000.\n",
      "OON Coinsurance % (80/20, 70/20, 60/40) NA NA 60/40, NA NA 60/40\n",
      "OON Coinsurance Limit ($5k, $10k $254) - Indiv. & Fam.\n",
      "\n",
      "COON Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. NA NA 9000/18000 NA NA 9000/8000.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "print(pytesseract.image_to_string(Image.open(\"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Screenshot (54).png\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6607fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from Screenshot (54).png:\n",
      "Group Heme: Aurora Products\n",
      "Group Number (or TBD): 1264561\n",
      "Request Date: 7214/2020\n",
      "Effective Date of NSB: 31/2020\n",
      "\n",
      "Please Note: All tems isied on this form are subject to review, Please highlight the NSB's in yellow,\n",
      "\n",
      "Benefits Currently in Place Requested Benefits\n",
      "\n",
      "Current Plan 4: Current Plan 2:\n",
      "0100001 BUY UP 0100002 CORE\n",
      "\n",
      "Base (Similar standard Plan) Tracking ID # (REQUIRED)\n",
      "+ Reach out to your Underwriter\n",
      "\n",
      "+ 4 14\n",
      "# of Employees Enrolled: 39 2 25 % 26 15\n",
      "Group State (NY, NJ, CT) cr cr cr. cr cr cr\n",
      "Market (Large or Small Group) Large Large Large Large Large Large\n",
      "Product (e.9: HHO, POS, Classic, Access, Direct, OxUSA, Value Option\n",
      "or EPO) HMO Huo HSA Huo HMO HSA\n",
      "Access (Gated or Non-Gated) Non-Gated Gated Non-Gated Non-Gated Gates Non-Gated\n",
      "Network (Freedom, Liberty, or Choice Pius) Freedom Freedom Freedom Freedom Freedom Freedom\n",
      "Current Carrier (Oxford or Competitor) Polaris Polaris Polaris Polaris\n",
      "\n",
      "Nore INE\n",
      "PCPISpecialist OV Copay 30/50 30/50 D8 100%\n",
      "ER Cost Share 450 150 pac\n",
      "IP: $260/Daq-$1500 Max (P: $250/0ay-$1500 Max\n",
      "after deductible ‘after deductible,\n",
      "OP:FS-Ded.then OP: FS-Ded,then\n",
      "\n",
      "$260Hisit  Hosp-Ded, then] $260Hvisit (Hosp Ded, then|\n",
      "Hospital Cost Share (Inpstient/Outpatient ‘$5000 $00 isi\n",
      "\n",
      "For Direct, EPO or Valve Option Products:\n",
      "IN Deductible - Individual & Family 2500/5000 2500/5000 5000/1000 2500/5000 2500/5000 5000/1000\n",
      "IN Coinsurance % (100, 90/10, 80/20 1000 4100/0 soo 4100/0 10070 90/10\n",
      "\n",
      "IN Coinsurance Limit ($Sk, $10k..) Indiv. & Fam.\n",
      "\n",
      "IN Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. 3000/6000 5000/1000 6250/12500 3000/6000 5000/1000 6250/12500\n",
      "Qut of Network (OON):\n",
      "\n",
      "OON Deductible - individual & Family NA NA. 5000/1000 NA. NA '5000/10000.\n",
      "OON Coinsurance % (80/20, 70/20, 60/40) NA NA 60/40, NA NA 60/40\n",
      "OON Coinsurance Limit ($5k, $10k $254) - Indiv. & Fam.\n",
      "\n",
      "COON Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. NA NA 9000/18000 NA NA 9000/8000.\n",
      "\n",
      "Text extracted from Screenshot (55).png:\n",
      "Plan Design:\n",
      "\n",
      "Office Copay:\n",
      "Pep:\n",
      "Specialist:\n",
      "\n",
      "ER Copay:\n",
      "Hospital Copay\n",
      "Inpatient:\n",
      "\n",
      "Outpatient Freestanding:\n",
      "Outpatient Hospital Setting:\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Sfoup Pinanctal invormation\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "KACTOHPGIRD.\n",
      "\n",
      "i.\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "tS Modified Cost Share:\n",
      "$2,850\n",
      "|\n",
      "None\n",
      "eT\n",
      "x | |\n",
      "\n",
      "Qut-of-Network\n",
      "\n",
      "Modified UCR:\n",
      "\n",
      "|\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Modified Cost Share:\n",
      "\n",
      "[ $2,850\n",
      "\n",
      "$5,700\n",
      "\n",
      "$5,850\n",
      "Cae >}\n",
      "\n",
      "Text extracted from Screenshot (56).png:\n",
      "Presci Pian\n",
      "Please see Rate Model for available plan combinations.\n",
      "\n",
      "x tracking is | (>\n",
      "\n",
      "Rx Copays: $5/$30/360 PDL CT Traditional\n",
      "Tier $5.copay Mail Order Copay\n",
      "Tier 2 C30 copay\n",
      "\n",
      "Modified Rx Cost Share:\n",
      "Tier 3 '$60 copay\n",
      "\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "Skilled Nursing: ST: 90Days x Outpatient Short Term Rehab: ST: 60 Visits\n",
      "\n",
      "Home Health Care: ST: 100 Days 5 Domestic Partners: ST: Not Coverea\n",
      "\n",
      "le\n",
      "\n",
      "ate\n",
      "\n",
      "OUTPATIENT CARE\n",
      "\n",
      "Primary Care Physician Office Visits\n",
      "Specialist Office Visits\n",
      "\n",
      "Virtual Visits\n",
      "\n",
      "Text extracted from Screenshot (58).png:\n",
      "UNITED HEALTHCARE UNDERWRITING FULLY INSURED APPROVAL FORM - For Internal Use Only\n",
      "\n",
      "FINAL SOLD RATES\n",
      "AGE/SEX 1.08\n",
      "ORIGINATING SITE Rhode Island EFFECTIV.4/1/2024_ THROUGH 12/31/2024\n",
      "GROUP NAME —_GREATER NASHUA MENTAL HEALTH CE ACCOUNTRyan King\n",
      "NEW BUSINESS/REINEW BUSINESS UNDERW#Kristen Patchel HP ALLIANCE GROUNo\n",
      "CARRIER REPLACE Y COMMISS $31.00 PESRVICE FEN/A # OF SITUS STATES N/A\n",
      "MULTLSITE (Y/N): ¥ COMMISSION NOTES SECOND SITUS STAWA\n",
      "DATE 1074/2023 AGGREG/N/A HP RATE TIER: = N/A\n",
      "‘COMPOSITE RENEVIN/A SUBJECT YES __NO CASE CHARACTERI__15\n",
      "‘OPTION 1\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium.\n",
      "‘Situs StateNH Subs __Mbrs imm, Fees Less Fees\n",
      "License (HEPO Benefit Plan CPI) Mod (Granite Advantage El EE Onl 34 $ 888.07\n",
      "Site(s)/Net National - Excl OOA_In Network Copays PCP $15/S15, SPC 25/825, OP $250/D&C EE +Sq 19] enone\n",
      "PlatfornvP Cirrus Granite Advai#e@e@fEoins/OOP_ Ded $5,000/S10,000 (Emb), Coins 80%, OO EE + CH 11 en\n",
      "‘Multiple P| Dual Option with 2 Out Of Network Ded N/A. Coins WA, OOP WA Family 26 Ean\n",
      "Coverage fN Min Value Pass ‘her/Riders UHCR Prem Totalsi____ 150] 261[ $ 782.56\n",
      "Legal Entit THFIC Drug Benefit $5/835/$70, 2 MO (Adv POLRx Plan: TH14\n",
      "Rating Group\n",
      "‘OPTION 2\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium\n",
      "Situs Stat (NH Subs __Mbrs imm, Fees ess Fees\n",
      "License (HEPO Benefit Plan CQFC Mo« (Granite Advantage EIHSA EE Onl 16] $782.62\n",
      "Site(s)/Net National - Excl OOA _In Network Copays PCP D&C, SPC D&C, OP D&C, IP D&C. UI EE+Sq 2| ee\n",
      "Platform/P Cirrus Granite Advaiite tet 0ins/OOP. Ded $4,000/$8,000 (Emb), Coins 100%, OO EE + CH 2| eon\n",
      "Multiple PDual Option with 1 Out Of Network Ded N/A. Coins WA, OOP WA Family 3 ‘eee\n",
      "Coverage fN Min Value Pass _“her/Riders UHCR Prem Totals/F, 23 46] $ 556.26\n",
      "Legal Entit THFIC Drug Benefit Med Ded, No Copay (AdRPBIgn: TH15-INT\n",
      "Rating Group\n",
      "‘GRAND TOTAL Premium Incl\n",
      "Subs __Mbrsimm, Fees\n",
      "EE Onl 110]\n",
      "Ee+sq 24\n",
      "Group has a 12.9% Rate Cap for next year EE +cH 13]\n",
      "Family 29\n",
      "Totalsi{___173] 307] $748.65\n",
      "\n",
      "Text extracted from Screenshot (59).png:\n",
      "[20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO OA? INBATENT SERVICES\n",
      "5%-$5000\n",
      "20% DEFERENTAL AN DUAL #3A HOSPITAL TERED OSNEWUSACO WNEATENT SERVICES 7 z 4 a0 Ey\n",
      "{a8 100s $1000T2COmAY\n",
      "'MNSORQ005 20% DEFERENTIAL PLAN QUAL #3A HOSPITAL TERED POS NOWUSACO NERTENT SERVICES 7 24 2m 2\n",
      "{a8 toon St000T2cOmA\n",
      "NSOBODIZS 20% DFFERENTAL PLAN DUAL 63 HOSPITAL TERED POS NENUSACOINBATENT SERVICES 1 204 on r)\n",
      "a TomnaT2COmA\n",
      "TWNSDOOENT 2003. aa. £70.01 TREAT SERVES i —I a0 @\n",
      "TMNSO00356 2025, eve, £0,05 INSATINT SERVICES 1 —r 000 Ey\n",
      "MNSOOOSSS_2003.Mano.PAT INBATIENT SERVICES 4 210m 0\n",
      "TMNSO200005_ CENTRAL MENUS GHTED HSA 100.8000 TES VE INBATINT SERVICES 1 4 ‘000 °\n",
      "TMNSOSO0BS_CHICASO 2014 XA.D16 INBATENT SERVICES + 24 009 0\n",
      "MNSOBD0DS_GHCASO-PO1EXA.D82 INSATINT SERVICES 1 a 500\n",
      "'MNS0300085 CHCAGO 2914 XA 083 INBATINT SERVICES 4 21 15000 +0\n",
      "MNSO300037 CHICAGO 2014 KA092 INBATINT SERVICES 4 I 0 +00\n",
      "\"MNS0300028 CHICAGO 201. XA. SONA INSATINT SERVICES + ——t 00 °\n",
      "TMNSOS000T7 CHICAGO 201A XA T2SNAVT INTENT SERVICES a —) 200 2\n",
      "'MNS03019 CHICAGO P14 XA S26NAY INBATIENT SERVICES o 2d 09 2\n",
      "'MNSO00014S GF LEGAL AD_OX NG CLACCESS 2014 008 INATINT SERVICES| 1 — 000 “a0\n",
      "'MNSO000TAE_GF LEGAL AID_OX NYLG CUACCESS2014,008 INATENT SERVICES 1 — 00 “00\n",
      "'MNSOQ00U5 GE LEGAL AD. OK NYLG-CL/ACCESS 2014 06 INBATENT SERVICES 1 — m0 “00\n",
      "TMNSOO00T46 GF ROCKEFELA UNIVERSITY. OKLNVIG, CACEESS 2OTA 101 ____NBATIENT SERV 4 2 4 000 00\n",
      "MNSOBOODTS 5050.3 INSATINT SERVICES 1 — 9 Fy\n",
      "‘wnsoa000%@ 5050.3 INBATIENT SERVES 1 a m0 Ey\n",
      "MNSO30008 CHS s350_100% INBATENT SERVICES 4 = 9 °\n",
      "TWNSOBOODE_LLHSATOO.ION INBATIENT SERVICES i —I ‘00 @\n",
      "MNSOSOO0E2 I HSATOO.TON INTENT SERVICES 1 24 00 E\n",
      "TWNSOQ0E6 LLHSATOOION INBATIENT SERVICES 5 24 00 10\n",
      "MNSO300065 ICHSATOON INBATIENT SERVICES| 1 I 000 2\n",
      "TVNSOSOO007_ICHSATOO.IN INSATIENT SERVICES 1 — 0 o\n",
      "‘MSOS0030_ICHSATOO.IN INGATINT SERVICES 5 — 09 =\n",
      "MNSo200034 LHSATO02 3000 INPATIENT SERVICES 1 2 4 0 0\n",
      "‘MNSO300085 _HSA1002.5000 24 INSATINT SERVICES 1 204 9 rr)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            print(f\"Text extracted from {filename}:\\n{text_data}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cce18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from Screenshot (54).png:\n",
      "Group Heme: Aurora Products\n",
      "Group Number (or TBD): 1264561\n",
      "Request Date: 7214/2020\n",
      "Effective Date of NSB: 31/2020\n",
      "\n",
      "Please Note: All tems isied on this form are subject to review, Please highlight the NSB's in yellow,\n",
      "\n",
      "Benefits Currently in Place Requested Benefits\n",
      "\n",
      "Current Plan 4: Current Plan 2:\n",
      "0100001 BUY UP 0100002 CORE\n",
      "\n",
      "Base (Similar standard Plan) Tracking ID # (REQUIRED)\n",
      "+ Reach out to your Underwriter\n",
      "\n",
      "+ 4 14\n",
      "# of Employees Enrolled: 39 2 25 % 26 15\n",
      "Group State (NY, NJ, CT) cr cr cr. cr cr cr\n",
      "Market (Large or Small Group) Large Large Large Large Large Large\n",
      "Product (e.9: HHO, POS, Classic, Access, Direct, OxUSA, Value Option\n",
      "or EPO) HMO Huo HSA Huo HMO HSA\n",
      "Access (Gated or Non-Gated) Non-Gated Gated Non-Gated Non-Gated Gates Non-Gated\n",
      "Network (Freedom, Liberty, or Choice Pius) Freedom Freedom Freedom Freedom Freedom Freedom\n",
      "Current Carrier (Oxford or Competitor) Polaris Polaris Polaris Polaris\n",
      "\n",
      "Nore INE\n",
      "PCPISpecialist OV Copay 30/50 30/50 D8 100%\n",
      "ER Cost Share 450 150 pac\n",
      "IP: $260/Daq-$1500 Max (P: $250/0ay-$1500 Max\n",
      "after deductible ‘after deductible,\n",
      "OP:FS-Ded.then OP: FS-Ded,then\n",
      "\n",
      "$260Hisit  Hosp-Ded, then] $260Hvisit (Hosp Ded, then|\n",
      "Hospital Cost Share (Inpstient/Outpatient ‘$5000 $00 isi\n",
      "\n",
      "For Direct, EPO or Valve Option Products:\n",
      "IN Deductible - Individual & Family 2500/5000 2500/5000 5000/1000 2500/5000 2500/5000 5000/1000\n",
      "IN Coinsurance % (100, 90/10, 80/20 1000 4100/0 soo 4100/0 10070 90/10\n",
      "\n",
      "IN Coinsurance Limit ($Sk, $10k..) Indiv. & Fam.\n",
      "\n",
      "IN Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. 3000/6000 5000/1000 6250/12500 3000/6000 5000/1000 6250/12500\n",
      "Qut of Network (OON):\n",
      "\n",
      "OON Deductible - individual & Family NA NA. 5000/1000 NA. NA '5000/10000.\n",
      "OON Coinsurance % (80/20, 70/20, 60/40) NA NA 60/40, NA NA 60/40\n",
      "OON Coinsurance Limit ($5k, $10k $254) - Indiv. & Fam.\n",
      "\n",
      "COON Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. NA NA 9000/18000 NA NA 9000/8000.\n",
      "\n",
      "Text extracted from Screenshot (55).png:\n",
      "Plan Design:\n",
      "\n",
      "Office Copay:\n",
      "Pep:\n",
      "Specialist:\n",
      "\n",
      "ER Copay:\n",
      "Hospital Copay\n",
      "Inpatient:\n",
      "\n",
      "Outpatient Freestanding:\n",
      "Outpatient Hospital Setting:\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Sfoup Pinanctal invormation\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "KACTOHPGIRD.\n",
      "\n",
      "i.\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "tS Modified Cost Share:\n",
      "$2,850\n",
      "|\n",
      "None\n",
      "eT\n",
      "x | |\n",
      "\n",
      "Qut-of-Network\n",
      "\n",
      "Modified UCR:\n",
      "\n",
      "|\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Modified Cost Share:\n",
      "\n",
      "[ $2,850\n",
      "\n",
      "$5,700\n",
      "\n",
      "$5,850\n",
      "Cae >}\n",
      "\n",
      "Text extracted from Screenshot (56).png:\n",
      "Presci Pian\n",
      "Please see Rate Model for available plan combinations.\n",
      "\n",
      "x tracking is | (>\n",
      "\n",
      "Rx Copays: $5/$30/360 PDL CT Traditional\n",
      "Tier $5.copay Mail Order Copay\n",
      "Tier 2 C30 copay\n",
      "\n",
      "Modified Rx Cost Share:\n",
      "Tier 3 '$60 copay\n",
      "\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "Skilled Nursing: ST: 90Days x Outpatient Short Term Rehab: ST: 60 Visits\n",
      "\n",
      "Home Health Care: ST: 100 Days 5 Domestic Partners: ST: Not Coverea\n",
      "\n",
      "le\n",
      "\n",
      "ate\n",
      "\n",
      "OUTPATIENT CARE\n",
      "\n",
      "Primary Care Physician Office Visits\n",
      "Specialist Office Visits\n",
      "\n",
      "Virtual Visits\n",
      "\n",
      "Text extracted from Screenshot (58).png:\n",
      "UNITED HEALTHCARE UNDERWRITING FULLY INSURED APPROVAL FORM - For Internal Use Only\n",
      "\n",
      "FINAL SOLD RATES\n",
      "AGE/SEX 1.08\n",
      "ORIGINATING SITE Rhode Island EFFECTIV.4/1/2024_ THROUGH 12/31/2024\n",
      "GROUP NAME —_GREATER NASHUA MENTAL HEALTH CE ACCOUNTRyan King\n",
      "NEW BUSINESS/REINEW BUSINESS UNDERW#Kristen Patchel HP ALLIANCE GROUNo\n",
      "CARRIER REPLACE Y COMMISS $31.00 PESRVICE FEN/A # OF SITUS STATES N/A\n",
      "MULTLSITE (Y/N): ¥ COMMISSION NOTES SECOND SITUS STAWA\n",
      "DATE 1074/2023 AGGREG/N/A HP RATE TIER: = N/A\n",
      "‘COMPOSITE RENEVIN/A SUBJECT YES __NO CASE CHARACTERI__15\n",
      "‘OPTION 1\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium.\n",
      "‘Situs StateNH Subs __Mbrs imm, Fees Less Fees\n",
      "License (HEPO Benefit Plan CPI) Mod (Granite Advantage El EE Onl 34 $ 888.07\n",
      "Site(s)/Net National - Excl OOA_In Network Copays PCP $15/S15, SPC 25/825, OP $250/D&C EE +Sq 19] enone\n",
      "PlatfornvP Cirrus Granite Advai#e@e@fEoins/OOP_ Ded $5,000/S10,000 (Emb), Coins 80%, OO EE + CH 11 en\n",
      "‘Multiple P| Dual Option with 2 Out Of Network Ded N/A. Coins WA, OOP WA Family 26 Ean\n",
      "Coverage fN Min Value Pass ‘her/Riders UHCR Prem Totalsi____ 150] 261[ $ 782.56\n",
      "Legal Entit THFIC Drug Benefit $5/835/$70, 2 MO (Adv POLRx Plan: TH14\n",
      "Rating Group\n",
      "‘OPTION 2\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium\n",
      "Situs Stat (NH Subs __Mbrs imm, Fees ess Fees\n",
      "License (HEPO Benefit Plan CQFC Mo« (Granite Advantage EIHSA EE Onl 16] $782.62\n",
      "Site(s)/Net National - Excl OOA _In Network Copays PCP D&C, SPC D&C, OP D&C, IP D&C. UI EE+Sq 2| ee\n",
      "Platform/P Cirrus Granite Advaiite tet 0ins/OOP. Ded $4,000/$8,000 (Emb), Coins 100%, OO EE + CH 2| eon\n",
      "Multiple PDual Option with 1 Out Of Network Ded N/A. Coins WA, OOP WA Family 3 ‘eee\n",
      "Coverage fN Min Value Pass _“her/Riders UHCR Prem Totals/F, 23 46] $ 556.26\n",
      "Legal Entit THFIC Drug Benefit Med Ded, No Copay (AdRPBIgn: TH15-INT\n",
      "Rating Group\n",
      "‘GRAND TOTAL Premium Incl\n",
      "Subs __Mbrsimm, Fees\n",
      "EE Onl 110]\n",
      "Ee+sq 24\n",
      "Group has a 12.9% Rate Cap for next year EE +cH 13]\n",
      "Family 29\n",
      "Totalsi{___173] 307] $748.65\n",
      "\n",
      "Text extracted from Screenshot (59).png:\n",
      "[20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO OA? INBATENT SERVICES\n",
      "5%-$5000\n",
      "20% DEFERENTAL AN DUAL #3A HOSPITAL TERED OSNEWUSACO WNEATENT SERVICES 7 z 4 a0 Ey\n",
      "{a8 100s $1000T2COmAY\n",
      "'MNSORQ005 20% DEFERENTIAL PLAN QUAL #3A HOSPITAL TERED POS NOWUSACO NERTENT SERVICES 7 24 2m 2\n",
      "{a8 toon St000T2cOmA\n",
      "NSOBODIZS 20% DFFERENTAL PLAN DUAL 63 HOSPITAL TERED POS NENUSACOINBATENT SERVICES 1 204 on r)\n",
      "a TomnaT2COmA\n",
      "TWNSDOOENT 2003. aa. £70.01 TREAT SERVES i —I a0 @\n",
      "TMNSO00356 2025, eve, £0,05 INSATINT SERVICES 1 —r 000 Ey\n",
      "MNSOOOSSS_2003.Mano.PAT INBATIENT SERVICES 4 210m 0\n",
      "TMNSO200005_ CENTRAL MENUS GHTED HSA 100.8000 TES VE INBATINT SERVICES 1 4 ‘000 °\n",
      "TMNSOSO0BS_CHICASO 2014 XA.D16 INBATENT SERVICES + 24 009 0\n",
      "MNSOBD0DS_GHCASO-PO1EXA.D82 INSATINT SERVICES 1 a 500\n",
      "'MNS0300085 CHCAGO 2914 XA 083 INBATINT SERVICES 4 21 15000 +0\n",
      "MNSO300037 CHICAGO 2014 KA092 INBATINT SERVICES 4 I 0 +00\n",
      "\"MNS0300028 CHICAGO 201. XA. SONA INSATINT SERVICES + ——t 00 °\n",
      "TMNSOS000T7 CHICAGO 201A XA T2SNAVT INTENT SERVICES a —) 200 2\n",
      "'MNS03019 CHICAGO P14 XA S26NAY INBATIENT SERVICES o 2d 09 2\n",
      "'MNSO00014S GF LEGAL AD_OX NG CLACCESS 2014 008 INATINT SERVICES| 1 — 000 “a0\n",
      "'MNSO000TAE_GF LEGAL AID_OX NYLG CUACCESS2014,008 INATENT SERVICES 1 — 00 “00\n",
      "'MNSOQ00U5 GE LEGAL AD. OK NYLG-CL/ACCESS 2014 06 INBATENT SERVICES 1 — m0 “00\n",
      "TMNSOO00T46 GF ROCKEFELA UNIVERSITY. OKLNVIG, CACEESS 2OTA 101 ____NBATIENT SERV 4 2 4 000 00\n",
      "MNSOBOODTS 5050.3 INSATINT SERVICES 1 — 9 Fy\n",
      "‘wnsoa000%@ 5050.3 INBATIENT SERVES 1 a m0 Ey\n",
      "MNSO30008 CHS s350_100% INBATENT SERVICES 4 = 9 °\n",
      "TWNSOBOODE_LLHSATOO.ION INBATIENT SERVICES i —I ‘00 @\n",
      "MNSOSOO0E2 I HSATOO.TON INTENT SERVICES 1 24 00 E\n",
      "TWNSOQ0E6 LLHSATOOION INBATIENT SERVICES 5 24 00 10\n",
      "MNSO300065 ICHSATOON INBATIENT SERVICES| 1 I 000 2\n",
      "TVNSOSOO007_ICHSATOO.IN INSATIENT SERVICES 1 — 0 o\n",
      "‘MSOS0030_ICHSATOO.IN INGATINT SERVICES 5 — 09 =\n",
      "MNSo200034 LHSATO02 3000 INPATIENT SERVICES 1 2 4 0 0\n",
      "‘MNSO300085 _HSA1002.5000 24 INSATINT SERVICES 1 204 9 rr)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store text with filenames as keys\n",
    "text_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            text_dict[filename] = text_data\n",
    "\n",
    "# Print or further process the organized text data\n",
    "for filename, text_data in text_dict.items():\n",
    "    print(f\"Text extracted from {filename}:\\n{text_data}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83d73255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to C:/Users/Shreshtha/Downloads/Tablee.csv\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# List to store rows of data\n",
    "data_rows = []\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            # Split the text into rows based on newline characters\n",
    "            rows = text_data.split('\\n')\n",
    "\n",
    "            # Assume the first row contains headers (modify as needed)\n",
    "            headers = rows[0].split('\\t')  # Assuming headers are tab-separated, change if needed\n",
    "\n",
    "            # Skip the first row (headers) and process the remaining rows\n",
    "            for row in rows[1:]:\n",
    "                values = row.split('\\t')  # Assuming values are tab-separated, change if needed\n",
    "                data_rows.append(dict(zip(headers, values)))\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "\n",
    "# Save the DataFrame to a CSV file or Excel sheet\n",
    "output_file = \"C:/Users/Shreshtha/Downloads/Tablee.csv\" # Change the output path as needed\n",
    "df.to_csv(output_file, index=False)\n",
    "# If you prefer Excel format, use df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Table saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b286a2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReadOnlyWorksheet' object has no attribute '_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m output_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Extract images from Excel and save them\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m extract_images_from_excel(input_excel_path, output_folder_path)\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mextract_images_from_excel\u001b[1;34m(input_excel, output_folder)\u001b[0m\n\u001b[0;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(sheet_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Iterate through images in the sheet\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39m_images:\n\u001b[0;32m     19\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimage\n\u001b[0;32m     20\u001b[0m     image_bytes \u001b[38;5;241m=\u001b[39m image_data\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ReadOnlyWorksheet' object has no attribute '_images'"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def extract_images_from_excel(input_excel, output_folder):\n",
    "    # Load the Excel workbook\n",
    "    workbook = load_workbook(input_excel, read_only=True)\n",
    "\n",
    "    # Iterate through each sheet in the workbook\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "\n",
    "        # Create a subfolder for each sheet\n",
    "        sheet_folder = os.path.join(output_folder, sheet_name)\n",
    "        os.makedirs(sheet_folder, exist_ok=True)\n",
    "\n",
    "        # Iterate through images in the sheet\n",
    "        for image in sheet._images:\n",
    "            image_data = image.image\n",
    "            image_bytes = image_data.decode('utf-8')\n",
    "            image_path = os.path.join(sheet_folder, f\"{sheet_name}_image_{image.index}.png\")\n",
    "\n",
    "            # Save the image using PIL\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(image_bytes)\n",
    "\n",
    "            print(f\"Image saved: {image_path}\")\n",
    "\n",
    "# Paths to your files\n",
    "input_excel_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "output_folder_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\"\n",
    "\n",
    "# Extract images from Excel and save them\n",
    "extract_images_from_excel(input_excel_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47824ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReadOnlyWorksheet' object has no attribute 'sheet_view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m output_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Extract images from Excel and save them\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m extract_images_from_excel(input_excel_path, output_folder_path)\n",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mextract_images_from_excel\u001b[1;34m(input_excel, output_folder)\u001b[0m\n\u001b[0;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(sheet_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check if the sheet has a drawing\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39msheet_view\u001b[38;5;241m.\u001b[39mdrawing:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Iterate through images in the sheet's drawing\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, drawing \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39msheet_view\u001b[38;5;241m.\u001b[39mdrawing\u001b[38;5;241m.\u001b[39mimages):\n\u001b[0;32m     21\u001b[0m         image_data \u001b[38;5;241m=\u001b[39m drawing\u001b[38;5;241m.\u001b[39mimage\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ReadOnlyWorksheet' object has no attribute 'sheet_view'"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def extract_images_from_excel(input_excel, output_folder):\n",
    "    # Load the Excel workbook\n",
    "    workbook = load_workbook(input_excel, read_only=True)\n",
    "\n",
    "    # Iterate through each sheet in the workbook\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "\n",
    "        # Create a subfolder for each sheet\n",
    "        sheet_folder = os.path.join(output_folder, sheet_name)\n",
    "        os.makedirs(sheet_folder, exist_ok=True)\n",
    "\n",
    "        # Check if the sheet has a drawing\n",
    "        if sheet.sheet_view.drawing:\n",
    "            # Iterate through images in the sheet's drawing\n",
    "            for idx, drawing in enumerate(sheet.sheet_view.drawing.images):\n",
    "                image_data = drawing.image\n",
    "                image_bytes = image_data.bytes\n",
    "                image_path = os.path.join(sheet_folder, f\"{sheet_name}_image_{idx + 1}.png\")\n",
    "\n",
    "                # Save the image using PIL\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(image_bytes)\n",
    "\n",
    "                print(f\"Image saved: {image_path}\")\n",
    "\n",
    "# Paths to your files\n",
    "input_excel_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "output_folder_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\"\n",
    "\n",
    "# Extract images from Excel and save them\n",
    "extract_images_from_excel(input_excel_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1931d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReadOnlyWorksheet' object has no attribute 'drawing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m input_excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m output_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m extract_images_from_excel(input_excel_path, output_folder_path)\n",
      "Cell \u001b[1;32mIn[23], line 25\u001b[0m, in \u001b[0;36mextract_images_from_excel\u001b[1;34m(input_excel, output_folder)\u001b[0m\n\u001b[0;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(sheet_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the folder if it doesn't exist\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Iterate through drawings (including images) in the sheet\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m drawing \u001b[38;5;129;01min\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39mdrawing:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drawing, Image):\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# Extract image bytes\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         image_bytes \u001b[38;5;241m=\u001b[39m drawing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mblob\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ReadOnlyWorksheet' object has no attribute 'drawing'"
     ]
    }
   ],
   "source": [
    "import openpyxl  # Library for working with Excel files\n",
    "from openpyxl.drawing.image import Image  # Class for handling images within Excel\n",
    "import os  # Library for working with file paths and directories\n",
    "\n",
    "def extract_images_from_excel(input_excel, output_folder):\n",
    "    \"\"\"Extracts images from an Excel workbook and saves them to a specified folder.\n",
    "\n",
    "    Args:\n",
    "        input_excel (str): Path to the input Excel workbook.\n",
    "        output_folder (str): Path to the output folder where images will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Excel workbook in read-only mode\n",
    "    workbook = openpyxl.load_workbook(input_excel, read_only=True)\n",
    "\n",
    "    # Iterate through each sheet in the workbook\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "\n",
    "        # Create a subfolder for each sheet within the output folder\n",
    "        sheet_folder = os.path.join(output_folder, sheet_name)\n",
    "        os.makedirs(sheet_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Iterate through drawings (including images) in the sheet\n",
    "        for drawing in sheet.drawing:\n",
    "            if isinstance(drawing, Image):\n",
    "                # Extract image bytes\n",
    "                image_bytes = drawing.image.blob\n",
    "                \n",
    "                # Generate a unique filename based on sheet name and image ID\n",
    "                image_filename = f\"{sheet_name}_image_{drawing.id}.png\"\n",
    "                image_path = os.path.join(sheet_folder, image_filename)\n",
    "\n",
    "                # Save the image using the extracted bytes\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(image_bytes)\n",
    "\n",
    "                print(f\"Image saved: {image_path}\")\n",
    "\n",
    "# Example usage:\n",
    "input_excel_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "output_folder_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\"\n",
    "\n",
    "extract_images_from_excel(input_excel_path, output_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c823b54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Worksheet' object has no attribute 'drawing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m output_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Extract images from Excel and save them\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m extract_images_from_excel(input_excel_path, output_folder_path)\n",
      "Cell \u001b[1;32mIn[24], line 13\u001b[0m, in \u001b[0;36mextract_images_from_excel\u001b[1;34m(input_excel, output_folder)\u001b[0m\n\u001b[0;32m     10\u001b[0m sheet \u001b[38;5;241m=\u001b[39m workbook\u001b[38;5;241m.\u001b[39mactive\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Iterate through each drawing in the sheet\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m drawing \u001b[38;5;129;01min\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39mdrawing:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drawing, Image):\n\u001b[0;32m     15\u001b[0m         image_bytes \u001b[38;5;241m=\u001b[39m drawing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mbytes\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Worksheet' object has no attribute 'drawing'"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "import os\n",
    "\n",
    "def extract_images_from_excel(input_excel, output_folder):\n",
    "    # Load the Excel workbook\n",
    "    workbook = load_workbook(input_excel)\n",
    "    \n",
    "    # Use the first sheet in the workbook (you might want to modify this based on your needs)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    # Iterate through each drawing in the sheet\n",
    "    for drawing in sheet.drawing:\n",
    "        if isinstance(drawing, Image):\n",
    "            image_bytes = drawing.image.bytes\n",
    "            image_path = os.path.join(output_folder, f\"image_{drawing.id}.png\")\n",
    "\n",
    "            # Save the image using PIL\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(image_bytes)\n",
    "\n",
    "            print(f\"Image saved: {image_path}\")\n",
    "\n",
    "# Paths to your files\n",
    "input_excel_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/Book1 (1).xlsx\"\n",
    "output_folder_path = \"C:/Users/Shreshtha/Downloads/hcu-main/hcu-main/\"\n",
    "\n",
    "# Extract images from Excel and save them\n",
    "extract_images_from_excel(input_excel_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aabd99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from Screenshot (54).png:\n",
      "Group Heme: Aurora Products\n",
      "Group Number (or TBD): 1264561\n",
      "Request Date: 7214/2020\n",
      "Effective Date of NSB: 31/2020\n",
      "\n",
      "Please Note: All tems isied on this form are subject to review, Please highlight the NSB's in yellow,\n",
      "\n",
      "Benefits Currently in Place Requested Benefits\n",
      "\n",
      "Current Plan 4: Current Plan 2:\n",
      "0100001 BUY UP 0100002 CORE\n",
      "\n",
      "Base (Similar standard Plan) Tracking ID # (REQUIRED)\n",
      "+ Reach out to your Underwriter\n",
      "\n",
      "+ 4 14\n",
      "# of Employees Enrolled: 39 2 25 % 26 15\n",
      "Group State (NY, NJ, CT) cr cr cr. cr cr cr\n",
      "Market (Large or Small Group) Large Large Large Large Large Large\n",
      "Product (e.9: HHO, POS, Classic, Access, Direct, OxUSA, Value Option\n",
      "or EPO) HMO Huo HSA Huo HMO HSA\n",
      "Access (Gated or Non-Gated) Non-Gated Gated Non-Gated Non-Gated Gates Non-Gated\n",
      "Network (Freedom, Liberty, or Choice Pius) Freedom Freedom Freedom Freedom Freedom Freedom\n",
      "Current Carrier (Oxford or Competitor) Polaris Polaris Polaris Polaris\n",
      "\n",
      "Nore INE\n",
      "PCPISpecialist OV Copay 30/50 30/50 D8 100%\n",
      "ER Cost Share 450 150 pac\n",
      "IP: $260/Daq-$1500 Max (P: $250/0ay-$1500 Max\n",
      "after deductible ‘after deductible,\n",
      "OP:FS-Ded.then OP: FS-Ded,then\n",
      "\n",
      "$260Hisit  Hosp-Ded, then] $260Hvisit (Hosp Ded, then|\n",
      "Hospital Cost Share (Inpstient/Outpatient ‘$5000 $00 isi\n",
      "\n",
      "For Direct, EPO or Valve Option Products:\n",
      "IN Deductible - Individual & Family 2500/5000 2500/5000 5000/1000 2500/5000 2500/5000 5000/1000\n",
      "IN Coinsurance % (100, 90/10, 80/20 1000 4100/0 soo 4100/0 10070 90/10\n",
      "\n",
      "IN Coinsurance Limit ($Sk, $10k..) Indiv. & Fam.\n",
      "\n",
      "IN Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. 3000/6000 5000/1000 6250/12500 3000/6000 5000/1000 6250/12500\n",
      "Qut of Network (OON):\n",
      "\n",
      "OON Deductible - individual & Family NA NA. 5000/1000 NA. NA '5000/10000.\n",
      "OON Coinsurance % (80/20, 70/20, 60/40) NA NA 60/40, NA NA 60/40\n",
      "OON Coinsurance Limit ($5k, $10k $254) - Indiv. & Fam.\n",
      "\n",
      "COON Out-of-Pocket Maximum (include ded.) - Indiv. & Fam. NA NA 9000/18000 NA NA 9000/8000.\n",
      "\n",
      "Text extracted from Screenshot (55).png:\n",
      "Plan Design:\n",
      "\n",
      "Office Copay:\n",
      "Pep:\n",
      "Specialist:\n",
      "\n",
      "ER Copay:\n",
      "Hospital Copay\n",
      "Inpatient:\n",
      "\n",
      "Outpatient Freestanding:\n",
      "Outpatient Hospital Setting:\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Sfoup Pinanctal invormation\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "KACTOHPGIRD.\n",
      "\n",
      "i.\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "No Charge after Deductible\n",
      "\n",
      "tS Modified Cost Share:\n",
      "$2,850\n",
      "|\n",
      "None\n",
      "eT\n",
      "x | |\n",
      "\n",
      "Qut-of-Network\n",
      "\n",
      "Modified UCR:\n",
      "\n",
      "|\n",
      "\n",
      "Single Deductible:\n",
      "Family Deductible:\n",
      "Coinsurance:\n",
      "\n",
      "Single M.0.0.P.\n",
      "\n",
      "Family M.0.0.P.\n",
      "\n",
      "Modified Cost Share:\n",
      "\n",
      "[ $2,850\n",
      "\n",
      "$5,700\n",
      "\n",
      "$5,850\n",
      "Cae >}\n",
      "\n",
      "Text extracted from Screenshot (56).png:\n",
      "Presci Pian\n",
      "Please see Rate Model for available plan combinations.\n",
      "\n",
      "x tracking is | (>\n",
      "\n",
      "Rx Copays: $5/$30/360 PDL CT Traditional\n",
      "Tier $5.copay Mail Order Copay\n",
      "Tier 2 C30 copay\n",
      "\n",
      "Modified Rx Cost Share:\n",
      "Tier 3 '$60 copay\n",
      "\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "Skilled Nursing: ST: 90Days x Outpatient Short Term Rehab: ST: 60 Visits\n",
      "\n",
      "Home Health Care: ST: 100 Days 5 Domestic Partners: ST: Not Coverea\n",
      "\n",
      "le\n",
      "\n",
      "ate\n",
      "\n",
      "OUTPATIENT CARE\n",
      "\n",
      "Primary Care Physician Office Visits\n",
      "Specialist Office Visits\n",
      "\n",
      "Virtual Visits\n",
      "\n",
      "Text extracted from Screenshot (58).png:\n",
      "UNITED HEALTHCARE UNDERWRITING FULLY INSURED APPROVAL FORM - For Internal Use Only\n",
      "\n",
      "FINAL SOLD RATES\n",
      "AGE/SEX 1.08\n",
      "ORIGINATING SITE Rhode Island EFFECTIV.4/1/2024_ THROUGH 12/31/2024\n",
      "GROUP NAME —_GREATER NASHUA MENTAL HEALTH CE ACCOUNTRyan King\n",
      "NEW BUSINESS/REINEW BUSINESS UNDERW#Kristen Patchel HP ALLIANCE GROUNo\n",
      "CARRIER REPLACE Y COMMISS $31.00 PESRVICE FEN/A # OF SITUS STATES N/A\n",
      "MULTLSITE (Y/N): ¥ COMMISSION NOTES SECOND SITUS STAWA\n",
      "DATE 1074/2023 AGGREG/N/A HP RATE TIER: = N/A\n",
      "‘COMPOSITE RENEVIN/A SUBJECT YES __NO CASE CHARACTERI__15\n",
      "‘OPTION 1\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium.\n",
      "‘Situs StateNH Subs __Mbrs imm, Fees Less Fees\n",
      "License (HEPO Benefit Plan CPI) Mod (Granite Advantage El EE Onl 34 $ 888.07\n",
      "Site(s)/Net National - Excl OOA_In Network Copays PCP $15/S15, SPC 25/825, OP $250/D&C EE +Sq 19] enone\n",
      "PlatfornvP Cirrus Granite Advai#e@e@fEoins/OOP_ Ded $5,000/S10,000 (Emb), Coins 80%, OO EE + CH 11 en\n",
      "‘Multiple P| Dual Option with 2 Out Of Network Ded N/A. Coins WA, OOP WA Family 26 Ean\n",
      "Coverage fN Min Value Pass ‘her/Riders UHCR Prem Totalsi____ 150] 261[ $ 782.56\n",
      "Legal Entit THFIC Drug Benefit $5/835/$70, 2 MO (Adv POLRx Plan: TH14\n",
      "Rating Group\n",
      "‘OPTION 2\n",
      "PRIMARY/SOLD PLAN Premium Incl Premium\n",
      "Situs Stat (NH Subs __Mbrs imm, Fees ess Fees\n",
      "License (HEPO Benefit Plan CQFC Mo« (Granite Advantage EIHSA EE Onl 16] $782.62\n",
      "Site(s)/Net National - Excl OOA _In Network Copays PCP D&C, SPC D&C, OP D&C, IP D&C. UI EE+Sq 2| ee\n",
      "Platform/P Cirrus Granite Advaiite tet 0ins/OOP. Ded $4,000/$8,000 (Emb), Coins 100%, OO EE + CH 2| eon\n",
      "Multiple PDual Option with 1 Out Of Network Ded N/A. Coins WA, OOP WA Family 3 ‘eee\n",
      "Coverage fN Min Value Pass _“her/Riders UHCR Prem Totals/F, 23 46] $ 556.26\n",
      "Legal Entit THFIC Drug Benefit Med Ded, No Copay (AdRPBIgn: TH15-INT\n",
      "Rating Group\n",
      "‘GRAND TOTAL Premium Incl\n",
      "Subs __Mbrsimm, Fees\n",
      "EE Onl 110]\n",
      "Ee+sq 24\n",
      "Group has a 12.9% Rate Cap for next year EE +cH 13]\n",
      "Family 29\n",
      "Totalsi{___173] 307] $748.65\n",
      "\n",
      "Text extracted from Screenshot (59).png:\n",
      "[20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO OA? INBATENT SERVICES\n",
      "5%-$5000\n",
      "20% DEFERENTAL AN DUAL #3A HOSPITAL TERED OSNEWUSACO WNEATENT SERVICES 7 z 4 a0 Ey\n",
      "{a8 100s $1000T2COmAY\n",
      "'MNSORQ005 20% DEFERENTIAL PLAN QUAL #3A HOSPITAL TERED POS NOWUSACO NERTENT SERVICES 7 24 2m 2\n",
      "{a8 toon St000T2cOmA\n",
      "NSOBODIZS 20% DFFERENTAL PLAN DUAL 63 HOSPITAL TERED POS NENUSACOINBATENT SERVICES 1 204 on r)\n",
      "a TomnaT2COmA\n",
      "TWNSDOOENT 2003. aa. £70.01 TREAT SERVES i —I a0 @\n",
      "TMNSO00356 2025, eve, £0,05 INSATINT SERVICES 1 —r 000 Ey\n",
      "MNSOOOSSS_2003.Mano.PAT INBATIENT SERVICES 4 210m 0\n",
      "TMNSO200005_ CENTRAL MENUS GHTED HSA 100.8000 TES VE INBATINT SERVICES 1 4 ‘000 °\n",
      "TMNSOSO0BS_CHICASO 2014 XA.D16 INBATENT SERVICES + 24 009 0\n",
      "MNSOBD0DS_GHCASO-PO1EXA.D82 INSATINT SERVICES 1 a 500\n",
      "'MNS0300085 CHCAGO 2914 XA 083 INBATINT SERVICES 4 21 15000 +0\n",
      "MNSO300037 CHICAGO 2014 KA092 INBATINT SERVICES 4 I 0 +00\n",
      "\"MNS0300028 CHICAGO 201. XA. SONA INSATINT SERVICES + ——t 00 °\n",
      "TMNSOS000T7 CHICAGO 201A XA T2SNAVT INTENT SERVICES a —) 200 2\n",
      "'MNS03019 CHICAGO P14 XA S26NAY INBATIENT SERVICES o 2d 09 2\n",
      "'MNSO00014S GF LEGAL AD_OX NG CLACCESS 2014 008 INATINT SERVICES| 1 — 000 “a0\n",
      "'MNSO000TAE_GF LEGAL AID_OX NYLG CUACCESS2014,008 INATENT SERVICES 1 — 00 “00\n",
      "'MNSOQ00U5 GE LEGAL AD. OK NYLG-CL/ACCESS 2014 06 INBATENT SERVICES 1 — m0 “00\n",
      "TMNSOO00T46 GF ROCKEFELA UNIVERSITY. OKLNVIG, CACEESS 2OTA 101 ____NBATIENT SERV 4 2 4 000 00\n",
      "MNSOBOODTS 5050.3 INSATINT SERVICES 1 — 9 Fy\n",
      "‘wnsoa000%@ 5050.3 INBATIENT SERVES 1 a m0 Ey\n",
      "MNSO30008 CHS s350_100% INBATENT SERVICES 4 = 9 °\n",
      "TWNSOBOODE_LLHSATOO.ION INBATIENT SERVICES i —I ‘00 @\n",
      "MNSOSOO0E2 I HSATOO.TON INTENT SERVICES 1 24 00 E\n",
      "TWNSOQ0E6 LLHSATOOION INBATIENT SERVICES 5 24 00 10\n",
      "MNSO300065 ICHSATOON INBATIENT SERVICES| 1 I 000 2\n",
      "TVNSOSOO007_ICHSATOO.IN INSATIENT SERVICES 1 — 0 o\n",
      "‘MSOS0030_ICHSATOO.IN INGATINT SERVICES 5 — 09 =\n",
      "MNSo200034 LHSATO02 3000 INPATIENT SERVICES 1 2 4 0 0\n",
      "‘MNSO300085 _HSA1002.5000 24 INSATINT SERVICES 1 204 9 rr)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store text with filenames as keys\n",
    "text_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            text_dict[filename] = text_data\n",
    "\n",
    "# Print or further process the organized text data\n",
    "for filename, text_data in text_dict.items():\n",
    "    print(f\"Text extracted from {filename}:\\n{text_data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1acce349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for Screenshot (54).png:\n",
      "              Filename                                               Text\n",
      "0  Screenshot (54).png  Group Heme: Aurora Products\\nGroup Number (or ...\n",
      "\n",
      "DataFrame for Screenshot (55).png:\n",
      "              Filename                                               Text\n",
      "0  Screenshot (55).png  Plan Design:\\n\\nOffice Copay:\\nPep:\\nSpecialis...\n",
      "\n",
      "DataFrame for Screenshot (56).png:\n",
      "              Filename                                               Text\n",
      "0  Screenshot (56).png  Presci Pian\\nPlease see Rate Model for availab...\n",
      "\n",
      "DataFrame for Screenshot (58).png:\n",
      "              Filename                                               Text\n",
      "0  Screenshot (58).png  UNITED HEALTHCARE UNDERWRITING FULLY INSURED A...\n",
      "\n",
      "DataFrame for Screenshot (59).png:\n",
      "              Filename                                               Text\n",
      "0  Screenshot (59).png  [20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store DataFrames with filenames as keys\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            df = pd.DataFrame({'Filename': [filename], 'Text': [text_data]})\n",
    "            dataframes_dict[filename] = df\n",
    "\n",
    "# Print or further process the DataFrames\n",
    "for filename, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame for {filename}:\\n{df}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c292fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for Screenshot (54).png:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screenshot (54).png</td>\n",
       "      <td>Group Heme: Aurora Products\\nGroup Number (or ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Filename                                               Text\n",
       "0  Screenshot (54).png  Group Heme: Aurora Products\\nGroup Number (or ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "DataFrame for Screenshot (55).png:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screenshot (55).png</td>\n",
       "      <td>Plan Design:\\n\\nOffice Copay:\\nPep:\\nSpecialis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Filename                                               Text\n",
       "0  Screenshot (55).png  Plan Design:\\n\\nOffice Copay:\\nPep:\\nSpecialis..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "DataFrame for Screenshot (56).png:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screenshot (56).png</td>\n",
       "      <td>Presci Pian\\nPlease see Rate Model for availab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Filename                                               Text\n",
       "0  Screenshot (56).png  Presci Pian\\nPlease see Rate Model for availab..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "DataFrame for Screenshot (58).png:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screenshot (58).png</td>\n",
       "      <td>UNITED HEALTHCARE UNDERWRITING FULLY INSURED A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Filename                                               Text\n",
       "0  Screenshot (58).png  UNITED HEALTHCARE UNDERWRITING FULLY INSURED A..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "DataFrame for Screenshot (59).png:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screenshot (59).png</td>\n",
       "      <td>[20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Filename                                               Text\n",
       "0  Screenshot (59).png  [20% DFFERENTAL PAN SIA HOSBTALTIERED NEUSACO ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store DataFrames with filenames as keys\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            df = pd.DataFrame({'Filename': [filename], 'Text': [text_data]})\n",
    "            dataframes_dict[filename] = df\n",
    "\n",
    "# Display the DataFrames\n",
    "for filename, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame for {filename}:\\n\")\n",
    "    display(df)\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e4c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from tabula-py) (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from tabula-py) (1.26.2)\n",
      "Collecting distro (from tabula-py)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shreshtha\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Downloading tabula_py-2.9.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.0 MB 825.8 kB/s eta 0:00:15\n",
      "   ---------------------------------------- 0.1/12.0 MB 1.0 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.1/12.0 MB 950.9 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/12.0 MB 1.0 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.3/12.0 MB 1.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.3/12.0 MB 1.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.3/12.0 MB 842.9 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.3/12.0 MB 819.2 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.3/12.0 MB 819.2 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.3/12.0 MB 819.2 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.4/12.0 MB 825.1 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.5/12.0 MB 823.7 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.5/12.0 MB 868.1 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.6/12.0 MB 901.1 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.6/12.0 MB 901.1 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.7/12.0 MB 969.4 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/12.0 MB 990.5 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/12.0 MB 952.3 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/12.0 MB 952.3 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/12.0 MB 952.3 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.9/12.0 MB 888.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.9/12.0 MB 844.2 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.9/12.0 MB 835.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/12.0 MB 849.5 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/12.0 MB 850.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.0/12.0 MB 859.7 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.1/12.0 MB 884.7 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.2/12.0 MB 889.8 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.2/12.0 MB 889.8 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.2/12.0 MB 866.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 865.0 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 867.0 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 852.2 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/12.0 MB 859.2 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/12.0 MB 865.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.5/12.0 MB 859.8 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.5/12.0 MB 862.2 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 868.2 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 870.3 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 873.8 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.7/12.0 MB 877.6 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.8/12.0 MB 889.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.8/12.0 MB 892.4 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.9/12.0 MB 896.9 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.9/12.0 MB 898.2 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.9/12.0 MB 892.9 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.0/12.0 MB 897.1 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.0/12.0 MB 901.1 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.0 MB 897.9 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.0 MB 891.6 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.1/12.0 MB 896.8 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.2/12.0 MB 890.8 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.3/12.0 MB 901.1 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.3/12.0 MB 906.3 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.3/12.0 MB 888.5 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.3/12.0 MB 889.4 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.4/12.0 MB 884.1 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/12.0 MB 885.4 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/12.0 MB 885.4 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.4/12.0 MB 885.4 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.5/12.0 MB 863.2 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.5/12.0 MB 848.1 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.5/12.0 MB 846.1 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.5/12.0 MB 838.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.0 MB 845.7 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.0 MB 850.9 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.7/12.0 MB 863.1 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.8/12.0 MB 880.4 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.8/12.0 MB 880.4 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.8/12.0 MB 864.5 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.9/12.0 MB 873.8 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 2.9/12.0 MB 879.1 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.0/12.0 MB 873.0 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.0/12.0 MB 867.8 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.0/12.0 MB 861.2 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.0/12.0 MB 862.3 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 859.6 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 854.0 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 854.0 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 854.0 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 824.7 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 822.0 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.2/12.0 MB 822.6 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.2/12.0 MB 823.2 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.3/12.0 MB 823.2 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.3/12.0 MB 832.0 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 837.5 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.5/12.0 MB 840.6 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.5/12.0 MB 845.9 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.5/12.0 MB 845.9 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.6/12.0 MB 842.4 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 3.6/12.0 MB 845.1 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.6/12.0 MB 837.6 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 3.7/12.0 MB 842.6 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.7/12.0 MB 845.2 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.7/12.0 MB 845.2 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.7/12.0 MB 845.2 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 3.8/12.0 MB 821.5 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 3.9/12.0 MB 847.2 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 3.9/12.0 MB 839.5 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.0/12.0 MB 840.5 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.0/12.0 MB 839.3 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.0/12.0 MB 836.8 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.1/12.0 MB 840.9 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.1/12.0 MB 839.1 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.2/12.0 MB 841.4 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.2/12.0 MB 843.8 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 848.1 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 849.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 849.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 849.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 849.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.3/12.0 MB 849.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.4/12.0 MB 827.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.4/12.0 MB 826.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.5/12.0 MB 825.9 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.5/12.0 MB 826.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.5/12.0 MB 826.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.5/12.0 MB 826.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.5/12.0 MB 826.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.7/12.0 MB 833.8 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.7/12.0 MB 834.6 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.8/12.0 MB 834.9 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.8/12.0 MB 834.8 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.9/12.0 MB 836.9 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.9/12.0 MB 837.7 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.9/12.0 MB 836.2 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 841.7 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 841.9 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 844.4 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 835.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.1/12.0 MB 834.2 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 830.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 830.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 825.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 817.2 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 816.4 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 816.4 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 816.4 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 816.4 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 800.5 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 799.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 793.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 793.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 788.6 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 789.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 784.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 778.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 778.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 778.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.4/12.0 MB 778.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.0 MB 775.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.0 MB 770.2 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.0 MB 768.0 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 765.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 765.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 765.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 758.4 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 758.4 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.6/12.0 MB 758.4 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.7/12.0 MB 747.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.7/12.0 MB 748.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.7/12.0 MB 746.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.7/12.0 MB 743.3 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.7/12.0 MB 743.3 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.7/12.0 MB 737.0 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 733.8 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 733.4 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 733.4 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 723.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 723.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 720.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 720.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 714.5 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.0 MB 710.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 708.6 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 708.6 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 705.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 705.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 705.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 696.3 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.9/12.0 MB 696.2 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.0 MB 694.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.0 MB 694.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.0 MB 690.9 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.0 MB 690.9 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.0 MB 690.9 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 683.4 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 683.4 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 683.4 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 672.5 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 672.5 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.1/12.0 MB 668.9 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.1/12.0 MB 667.8 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.1/12.0 MB 666.5 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.2/12.0 MB 666.5 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.2/12.0 MB 672.0 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.0 MB 674.1 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.0 MB 674.1 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.0 MB 669.6 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.0 MB 667.3 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.3/12.0 MB 669.5 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.4/12.0 MB 666.1 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.4/12.0 MB 666.1 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.5/12.0 MB 671.3 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.0 MB 676.5 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 6.6/12.0 MB 681.7 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 681.6 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 681.6 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 680.2 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 679.2 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 677.9 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 677.8 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 677.7 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 6.9/12.0 MB 684.7 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.0/12.0 MB 686.6 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.0/12.0 MB 688.5 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 692.3 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 692.3 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 690.1 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 690.1 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 683.8 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.2/12.0 MB 684.6 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.3/12.0 MB 691.3 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 7.4/12.0 MB 696.0 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 7.5/12.0 MB 701.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.5/12.0 MB 704.3 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.5/12.0 MB 704.3 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.5/12.0 MB 697.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.6/12.0 MB 700.7 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.6/12.0 MB 700.5 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.7/12.0 MB 702.2 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 7.7/12.0 MB 702.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 7.8/12.0 MB 708.3 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 7.9/12.0 MB 712.7 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.0/12.0 MB 716.1 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.0/12.0 MB 716.8 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.1/12.0 MB 720.4 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.1/12.0 MB 716.3 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.1/12.0 MB 717.0 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.2/12.0 MB 717.6 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.2/12.0 MB 717.3 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.2/12.0 MB 718.1 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.3/12.0 MB 717.7 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.3/12.0 MB 719.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 8.4/12.0 MB 724.2 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.5/12.0 MB 726.5 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.5/12.0 MB 729.0 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.5/12.0 MB 729.0 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.6/12.0 MB 729.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.6/12.0 MB 729.8 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 732.0 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 733.5 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 732.4 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 730.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 730.0 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 728.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 728.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 728.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 727.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 9.0/12.0 MB 727.6 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 9.0/12.0 MB 729.8 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 9.1/12.0 MB 733.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.1/12.0 MB 732.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.2/12.0 MB 731.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.2/12.0 MB 732.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.3/12.0 MB 735.8 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.3/12.0 MB 733.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.4/12.0 MB 735.9 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.4/12.0 MB 737.9 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.5/12.0 MB 741.6 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.6/12.0 MB 745.2 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.6/12.0 MB 746.6 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.7/12.0 MB 747.9 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.8/12.0 MB 750.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.9/12.0 MB 754.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.9/12.0 MB 755.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 9.9/12.0 MB 755.9 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/12.0 MB 756.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.1/12.0 MB 760.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.2/12.0 MB 763.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.2/12.0 MB 763.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.3/12.0 MB 767.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.4/12.0 MB 768.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.4/12.0 MB 769.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 767.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 767.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 762.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.6/12.0 MB 772.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.6/12.0 MB 771.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 773.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.8/12.0 MB 780.4 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.9/12.0 MB 777.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.0 MB 774.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.0 MB 783.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.1/12.0 MB 784.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.1/12.0 MB 784.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 783.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 783.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 780.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 780.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 775.7 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.2/12.0 MB 773.9 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.3/12.0 MB 772.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.0 MB 773.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.4/12.0 MB 778.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.0 MB 779.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.0 MB 783.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.0 MB 786.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.7/12.0 MB 787.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 788.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 792.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 793.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 793.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 788.8 kB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, tabula-py\n",
      "Successfully installed distro-1.9.0 tabula-py-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba84dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (54).png: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (55).png: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (56).png: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (58).png: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (59).png: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "import tabula\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract tables from an image using tabula-py\n",
    "def extract_tables_from_image(image_path):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(image_path, pages='all', multiple_tables=True)\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store DataFrames with filenames as keys\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "        tables = extract_tables_from_image(image_path)\n",
    "\n",
    "        if tables:\n",
    "            for idx, table in enumerate(tables):\n",
    "                df = pd.DataFrame(table)\n",
    "                key = f\"{filename}_table_{idx + 1}\"\n",
    "                dataframes_dict[key] = df\n",
    "\n",
    "# Display the DataFrames\n",
    "for key, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame for {key}:\\n\")\n",
    "    display(df)\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8521a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (54).png: File format not supported\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (55).png: File format not supported\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (56).png: File format not supported\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (58).png: File format not supported\n",
      "Error extracting tables from image C:/Users/Shreshtha/Downloads/Images Directory\\Screenshot (59).png: File format not supported\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "import camelot\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract tables from an image using camelot-py\n",
    "def extract_tables_from_image(image_path):\n",
    "    try:\n",
    "        tables = camelot.read_pdf(image_path, flavor='stream', pages='all')\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dictionary to store DataFrames with filenames as keys\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_text_from_image(image_path)\n",
    "        tables = extract_tables_from_image(image_path)\n",
    "\n",
    "        if tables:\n",
    "            for idx, table in enumerate(tables):\n",
    "                df = table.df\n",
    "                key = f\"{filename}_table_{idx + 1}\"\n",
    "                dataframes_dict[key] = df\n",
    "\n",
    "# Display the DataFrames\n",
    "for key, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame for {key}:\\n\")\n",
    "    display(df)\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43acedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from Screenshot (54).png:\n",
      "Group Nam¢\n",
      "Group Mumber (or TBD}:\n",
      "Request Date:\n",
      "\n",
      "Effective Date of NSB:\n",
      "\n",
      "Aurora Products.\n",
      "\n",
      "1284561\n",
      "\n",
      "24/2020.\n",
      "\n",
      "314:2020\n",
      "\n",
      "Pleage Note: Alltems Isted c7 ths form are sub-ect 7a review Please highlight the NSW In vellow,\n",
      "\n",
      "Benefits Currently wy Place\n",
      "\n",
      "Requested Benefits\n",
      "\n",
      "General information: Requirea\n",
      "\n",
      "Current Pian 4:\n",
      "0100001 BUY UP\n",
      "\n",
      "Current Plan 2:\n",
      "0100002 CORE\n",
      "\n",
      "Current Plan 3:\n",
      "0100004 BASE\n",
      "\n",
      "Current Plan 4:\n",
      "0100001 BUY UP\n",
      "\n",
      "Current Plan 2:\n",
      "0100002 CORE\n",
      "\n",
      "Current Plan 3:\n",
      "0100004 BASE\n",
      "\n",
      "sues Fo 5 Fo TOT acon ms MONRO IIR To ETO cs an\n",
      "PeP'specalst Ov Cops i f [a |e CaiB0e\n",
      "Psat cect] geet cet\n",
      "Pia cater ses | gosiiisu Hosp: Ded the] s260hsit Hose Ded, the]\n",
      "TROT\n",
      "ow Ovt-of.Pocket Maxurun anode det). mae. & ram na ua scoa'000 wa ha canvyieove\n",
      "\n",
      "Text extracted from Screenshot (55).png:\n",
      "Sroup Einancial information\n",
      "Link te PDML;\n",
      "Please see Rate Model for available ptan combinations\n",
      "Plan Design: KACTOHP SIRO\n",
      "In-Network Qut-of-Network\n",
      "Modinea UCR:\n",
      "\n",
      "Speciatist: No Cherge after Deductible\n",
      "ER Copay: No Charge aher Deductible\n",
      "Hospital Copay\n",
      "\n",
      "Ns Modified Cost Share:\n",
      "\n",
      "single Deduct: oa Single Dede aa\n",
      "Family Deductible: CoE] — Famity Deductibt YET ]\n",
      "amily W008 = Family W0.0P sim\n",
      "Flnancal Accumulation Pood: cae\n",
      "\n",
      "Please see Rate Model for available plan combinations\n",
      "\n",
      "Text extracted from Screenshot (56).png:\n",
      "Praseription Plan\n",
      "\n",
      "Rate Modal for available plan combinations\n",
      "\n",
      "Rx Tracking ID:\n",
      "Rx Copays: 508 Pot Tes anor\n",
      "Tier $5 copa Mail Order Copay Te copay\n",
      "Tier 2 $30 copay\n",
      "Modified Rx Cost Share:\n",
      "\n",
      "Tier 3 SEP copay\n",
      "\n",
      "Piders\n",
      "Please sae Rate Modal for available plan combinations\n",
      "\n",
      "Skitled Nursing: sre nyt ~ ‘Omnpatient Shot Term Rehab: STC se\n",
      "\n",
      "Home Health Care:\n",
      "\n",
      "st napa x Domestic Partners:\n",
      "\n",
      "Benefit Modiffeation\n",
      "\n",
      "QUTPATIENT CARE\n",
      "\n",
      "Primary Care Physician Office Visits No Charge after Deductible\n",
      "\n",
      "Spectalist Office Visits Ho Chatge attes Deductibie\n",
      "\n",
      "Viewat Visite Ho Chacge after Deductible\n",
      "\n",
      "Outpatient Surgery - Hospital Setting No Charge after Deductible\n",
      "\n",
      "Outpationt Surgery - Freestanding Facitny Wo Charge ofter Deductibie\n",
      "\n",
      "Designated Diagnostic Provider Laboratory Services No Charge ater Deduetible\n",
      "\n",
      "Mon. Deslgnated Diagnostic Provider Lebormtory Services Deductible & 50% Colneurance No Charge after Deductible\n",
      "Radiology Services No Charge ser Dedctibie\n",
      "\n",
      "Text extracted from Screenshot (58).png:\n",
      "UNITED HEALTHCARE UNDERWRITING FULLY INSURED APPROVAL FORM - For Internal Use Only\n",
      "\n",
      "ORIGINATING SITE Rhode Island\n",
      "\n",
      "FINAL SOLD RATES\n",
      "\n",
      "EFFECTIV1. 4/2024 THROUGH 12/34/2024\n",
      "\n",
      "GROUP TAME\n",
      "\n",
      "GREATER NASHUA ME ITAL JEALTH CE ACCOUNTRyan K ni\n",
      "\n",
      "NEW BUSINESS/REINE} SUSILESS:\n",
      "CARRIER REPLACE Y\n",
      "\n",
      "MULTLSITE (YH)\n",
      "\n",
      "DATE 104\n",
      "\n",
      "UNDERW Kristen Pavel\n",
      "COMMISSE31 30 >I\n",
      "COMMISSION HOTES\n",
      "\n",
      "AGE'SEX\n",
      "\n",
      "08\n",
      "\n",
      "HP ALLIANCE GROUT.\n",
      "# OF SITUS STATES I. 4\n",
      "SECOND SITUS STAT, A\n",
      "\n",
      "2003 AGGREG/HEA HP RATE TIER hea\n",
      "COMPOSITE RENEV Ii SUBJECT YES __ 113 CASE CHARACTERK__15\n",
      "OPTION 1\n",
      "PRIMARY!SOLD CLAN Premium Incl Premium\n",
      "Situs StariH Subs ___Mbrsimm Fees Less Fees\n",
      "License IHEP Benefit Plan CFU [oo (Gratite Advantage EL EE Onl 5A § 83507\n",
      "Site(s)'Netlationa -Excl OOA In Network Cooays PCP $15 $18 SPC $2525 OP S250DRC EE +S 19 Brenner]\n",
      "Platfor’P Cirrus Granite Adysiee@eGE oins/OOP Ded 9 030-510 060 Ema; Cons 80% OC EE +Ch 1 Emonand\n",
      "Multiple PiCual Qstion wth 2 Out Of Network Ded A Coins HA OOP 1A Family 26 Eman\n",
      "Coverage fl Min Value Fass her Riders UHR Pram Totals/F| 160) 261] § 752 26\n",
      "Legal Entit THFIC Orug Benefit $F 532/970 210 Ace PMI Ry Plan THY\n",
      "Rating Group\n",
      "OPTION 2\n",
      "PRIMARY!SOED PLAN Premium Incl Premium,\n",
      "Situs State Subs __Mbrsimm_ Fees ess Fees\n",
      "License (HEP Benefit Plan COFC MatiGranite Advantage EIFSA EE Only 1g § 752 62\n",
      "Site(sMetLaticna -Excl GOA _ In Network Coaays PCP D&C SPC DRC OPC&C IFDAC UK EE+S 2 Eronaed\n",
      "PlatformP Cirrus Granite Adsadte@eG oins/OOP. Ded $4 000-58 20¢ (Ene. Coins 190% OC EE ~Ch 2 Enemas]\n",
      "Multiple PiCual Ostion » th 7 Out Of Network Ded A Coins HA OO? 1A Family 3 Enid\n",
      "Coverage fh Min Value Fass _her‘Riders URGR Prem Totals/F| 23\n",
      "Legal Entit THFIS. Drug Benefit Med Ded No Capsy iAdRPBlan —H751°TT\n",
      "Rating Group\n",
      "GRAND TOTAL Premium Incl\n",
      "Subs Mbrsimim_Fees\n",
      "EE Only 119\n",
      "EE+S 21\n",
      "Group has 9 12.6) Rate Cap fnr next year EE+c 13]\n",
      "Family 23\n",
      "Totals/F| 173 307| $748 65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Path to the directory containing the images\n",
    "images_directory = \"C:/Users/Shreshtha/Downloads/Images Directory\"\n",
    "\n",
    "# Function to extract text from an image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(Image.open(image_path))\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract tabular data from an image using OpenCV\n",
    "def extract_tabular_data(image_path):\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use a simple thresholding technique to convert to binary image\n",
    "    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Use pytesseract to extract text from the binary image\n",
    "    text_data = pytesseract.image_to_string(Image.fromarray(binary))\n",
    "\n",
    "    return text_data.strip()\n",
    "\n",
    "# Dictionary to store text with filenames as keys\n",
    "text_dict = {}\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(images_directory):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(images_directory, filename)\n",
    "        text_data = extract_tabular_data(image_path)\n",
    "\n",
    "        if text_data:\n",
    "            text_dict[filename] = text_data\n",
    "\n",
    "# Print or further process the organized text data\n",
    "for filename, text_data in text_dict.items():\n",
    "    print(f\"Text extracted from {filename}:\\n{text_data}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
